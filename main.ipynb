{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tabulate import tabulate  # for rendering board\n",
    "from enum import Enum\n",
    "from random import randint, choice\n",
    "from copy import copy, deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment possible actions: swipe to left, right, up, down\n",
    "class Action(Enum):\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "    Left = 1\n",
    "    Right = 2\n",
    "    Up = 3\n",
    "    Down = 4\n",
    "\n",
    "class GameEnvironment:\n",
    "    def __init__(self, board_size=3, target=64, initial_state=None, calculate_possible_states=True):\n",
    "        # dynamic board size\n",
    "        self.board_size = board_size\n",
    "        self.won = False\n",
    "        if initial_state == None:\n",
    "            # start with empty board\n",
    "            self.__initial_state = np.zeros([board_size, board_size] ,int)\n",
    "            # generate 2 random tiles on board\n",
    "            for i in range(2):\n",
    "                self.__initial_state = self.__generate_new_tile(self.__initial_state)\n",
    "            \n",
    "        else:\n",
    "            # copy to prevent aliassing\n",
    "            self.__initial_state = copy(initial_state)\n",
    "\n",
    "        self.target = target\n",
    "        self.__state = self.__initial_state\n",
    "        self.__possible_states = []\n",
    "        # during performance assessment, it's not required\n",
    "        if calculate_possible_states:\n",
    "            self.__calculate_possible_states_from_different_initial_states(self.__initial_state)\n",
    "        # print(len(self.__possible_states))\n",
    "        \n",
    "    def __calculate_possible_states_from_different_initial_states(self, state:np.ndarray = None):\n",
    "        for n in range(15):\n",
    "            # generate new initial state\n",
    "            initial_state = np.zeros([self.board_size, self.board_size] ,int)\n",
    "            # generate 2 random tiles on board\n",
    "            for i in range(2):\n",
    "                initial_state = self.__generate_new_tile(self.__initial_state)\n",
    "            \n",
    "            self.__calculate_possible_states(initial_state, depth=2)\n",
    "            \n",
    "        \n",
    "    \n",
    "    def __calculate_possible_states(self, state:np.ndarray=None, action=None, depth=5):\n",
    "        tile_2_depth = copy(depth)\n",
    "        tile_4_depth = copy(depth)\n",
    "        \n",
    "        if state is None:\n",
    "            state = self.__initial_state\n",
    "        \n",
    "        if action == None:\n",
    "            possible_actions = self.get_possible_actions(all_actions=True)\n",
    "        else:\n",
    "            possible_actions = [action]\n",
    "            \n",
    "        # possible states returned to get_transition_prob\n",
    "        singleton_possible_states = []\n",
    "            \n",
    "        # get all possible actions \n",
    "        for action in possible_actions:\n",
    "        # calculate the outcome state\n",
    "            outcome_state = self.__calculate_transition(action, state, new_tile=False)\n",
    "        # append to the self.__possible_states (to be used in the utility function)\n",
    "            empty_tiles = self.get_empty_tiles(outcome_state)\n",
    "            if len(empty_tiles) > 0:\n",
    "                # generate new tile at random empty cell\n",
    "                for tile in empty_tiles:\n",
    "                    new_state = deepcopy(outcome_state)   \n",
    "                    #if random generated tile is 2\n",
    "                    new_state[tile[0]][tile[1]] = 2\n",
    "                    temp_state = deepcopy(new_state)\n",
    "                    if not self.contains(self.__possible_states, temp_state):\n",
    "                        self.__possible_states.append(temp_state)\n",
    "                    \n",
    "                    singleton_possible_states.append(temp_state)\n",
    "                    if not self.is_done(temp_state) and tile_2_depth > 0:\n",
    "                        for calculated_step in self.__calculate_possible_states(deepcopy(temp_state),depth = tile_2_depth - 1):\n",
    "                            singleton_possible_states.append(calculated_step)\n",
    "                    \n",
    "                    #if random generated tile is 4\n",
    "                    new_state[tile[0]][tile[1]] = 4\n",
    "                    temp_state = deepcopy(new_state)\n",
    "                    if not self.contains(self.__possible_states, temp_state):\n",
    "                        self.__possible_states.append(temp_state)\n",
    "                    \n",
    "                    singleton_possible_states.append(temp_state)\n",
    "                    if not self.is_done(temp_state) and tile_4_depth > 0:\n",
    "                        for calculated_step in self.__calculate_possible_states(deepcopy(temp_state),depth = tile_4_depth - 1):\n",
    "                            singleton_possible_states.append(calculated_step)\n",
    "        \n",
    "\n",
    "        # print(f\"possible singleton states: {len(singleton_possible_states)}\")\n",
    "        return singleton_possible_states\n",
    "        \n",
    "    def reset(self):\n",
    "        self.won = False\n",
    "        self.__state = self.__initial_state\n",
    "        return self.__state\n",
    "\n",
    "    # perform action on environment\n",
    "    def __calculate_transition(self, action:Action, state:np.ndarray=None, new_tile:bool = True):        \n",
    "        if state is None:\n",
    "            new_state = self.__state\n",
    "        else:\n",
    "            new_state = deepcopy(state)\n",
    "            \n",
    "        if self.is_done(state):\n",
    "            return new_state\n",
    "\n",
    "        # 1. change the state to reflect the move by the agent,\n",
    "        # 2. merge same value tiles\n",
    "\n",
    "        # swipe to left\n",
    "        if action == Action.Left:\n",
    "            new_state = self.swipeToLeft(new_state)\n",
    "            new_state= self.mergeToLeft(new_state)\n",
    "        # swipe to right\n",
    "        elif action == Action.Right:\n",
    "            new_state = self.swipeToRight(new_state)\n",
    "            new_state = self.mergeToRight(new_state)\n",
    "        elif action == Action.Up:\n",
    "            # take transpose, swipe, then re-take transpose\n",
    "            temp_state = self.transpose(new_state)\n",
    "            temp_state = self.swipeToLeft(temp_state)\n",
    "            temp_state = self.mergeToLeft(temp_state)\n",
    "            new_state = self.transpose(temp_state)\n",
    "        elif action == Action.Down:\n",
    "            # take transpose\n",
    "            temp_state = self.transpose(new_state)\n",
    "            temp_state = self.swipeToRight(temp_state)\n",
    "            temp_state = self.mergeToRight(temp_state)\n",
    "            new_state = self.transpose(temp_state)\n",
    "\n",
    "        # 3. generate a new tile on empty cells\n",
    "        if new_tile:\n",
    "            new_state = self.__generate_new_tile(new_state)\n",
    "            \n",
    "        return new_state\n",
    "\n",
    "    def __generate_new_tile(self, state):\n",
    "        new_state = deepcopy(state)\n",
    "        empty_state = self.get_empty_tiles(new_state)\n",
    "\n",
    "        if len(empty_state) > 0:\n",
    "            # possible generated tile values\n",
    "            possible_gen_tiles = [2, 4]\n",
    "            # generate new tile at random empty cell\n",
    "            row, col = choice(empty_state)\n",
    "            new_state[row][col] = possible_gen_tiles[randint(0, 1)]\n",
    "        return new_state\n",
    "        \n",
    "    def swipeToLeft(self, state):\n",
    "        for i in range(self.board_size):\n",
    "            for j in range(self.board_size - 1):\n",
    "                # [0,2,2]: if current cell is empty, swap with the right one\n",
    "                if state[i][j] == 0:\n",
    "\n",
    "                    # k is the offset of the first found tile\n",
    "                    for k in range(1, self.board_size - j):\n",
    "                        if state[i][j + k] != 0:\n",
    "                            self.swap(state, i, j, i, j + k)\n",
    "                            break\n",
    "        return state\n",
    "\n",
    "    def mergeToLeft(self, state):\n",
    "        # merge same tiles together\n",
    "        for i in range(self.board_size):\n",
    "            for j in range(self.board_size - 1):\n",
    "                current_tile = state[i][j]\n",
    "                if current_tile != 0:\n",
    "                    right_tile = state[i][j + 1]\n",
    "                    if right_tile == current_tile:\n",
    "                        # merge same tiles together\n",
    "                        state[i][j] = current_tile * 2\n",
    "                        state[i][j + 1] = 0\n",
    "                        # shift to the left other tiles\n",
    "                        for k in range(j + 1, self.board_size - 1):\n",
    "                            # current tile equal right tile\n",
    "                            state[i][j + k] = state[i][j + k + 1]\n",
    "\n",
    "                        # last cell is empty\n",
    "                        state[i][self.board_size - 1] = 0\n",
    "\n",
    "        return state\n",
    "\n",
    "    def swipeToRight(self, state):\n",
    "        for i in range(self.board_size):\n",
    "            for j in reversed(range(1, self.board_size)):\n",
    "                # [2,2,0]: if current cell is empty, swap with the left one\n",
    "                if state[i][j] == 0:\n",
    "\n",
    "                    # k is the offset of the first found tile\n",
    "                    for k in range(1, j + 1):\n",
    "                        if state[i][j - k] != 0:\n",
    "\n",
    "                            self.swap(state, i, j, i, j - k)\n",
    "                            break\n",
    "        return state\n",
    "\n",
    "    def mergeToRight(self, state):\n",
    "        # merge same tiles together\n",
    "        for i in range(self.board_size):\n",
    "            for j in reversed(range(1, self.board_size)):\n",
    "                current_tile = state[i][j]\n",
    "                if current_tile != 0:\n",
    "                    left_tile = state[i][j - 1]\n",
    "                    if left_tile == current_tile:\n",
    "                        # merge same tiles together\n",
    "                        state[i][j] = current_tile * 2\n",
    "                        state[i][j - 1] = 0\n",
    "                        # shift to the right other tiles\n",
    "                        for k in reversed(range(1, j - 1)):\n",
    "                            # current tile equal right tile\n",
    "                            state[i][j - k] = state[i][j - k - 1]\n",
    "                        # first cell is empty\n",
    "                        state[i][0] = 0\n",
    "\n",
    "        return state\n",
    "\n",
    "    def transpose(self, array):\n",
    "        transposed_array = np.transpose(array)\n",
    "        return transposed_array\n",
    "\n",
    "    def swap(self, state, x1, y1, x2, y2):\n",
    "        # x and y are the position of the board matrix\n",
    "        z = state[x1][y1]\n",
    "        state[x1][y1] = state[x2][y2]\n",
    "        state[x2][y2] = z\n",
    "\n",
    "    # unit step on environment\n",
    "    def step(self, action):\n",
    "        old_state = self.__state\n",
    "        # state after agent action\n",
    "        self.__state = self.__calculate_transition(action)\n",
    "        observation = self.__state  # environment is fully observable\n",
    "        done = self.is_done()\n",
    "        reward = self.get_reward(self.__state)\n",
    "        info = {}  # optional    debug info\n",
    "        return observation, done, reward, info\n",
    "\n",
    "    # render environment (board) on CLI\n",
    "    def render(self,state:np.ndarray = None):\n",
    "        if state is None:\n",
    "            state = deepcopy(self.__state)\n",
    "        print_state = []\n",
    "        for item in state:\n",
    "            print_state.append(['' if x==0 else x for x in item])\n",
    "        print(tabulate(print_state, tablefmt=\"grid\"))\n",
    "\n",
    "    # =========================================================\n",
    "    # public functions for agent to calculate optimal policy\n",
    "    # =========================================================\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.__state\n",
    "\n",
    "    def get_possible_states(self):\n",
    "        return self.__possible_states\n",
    "\n",
    "    # get index of empty cells\n",
    "    def get_empty_tiles(self, state=None):\n",
    "        if state is None:\n",
    "            state = self.__state\n",
    "        empty_cells = []\n",
    "        for i in range(self.board_size):\n",
    "            for j in range(self.board_size):\n",
    "                if state[i][j] == 0:\n",
    "                    empty_cells.append([i, j])\n",
    "\n",
    "        return empty_cells\n",
    "\n",
    "    def get_possible_actions(self, old_state:np.ndarray = None, all_actions=False):\n",
    "        if old_state is None:\n",
    "            old_state = copy(self.__initial_state)\n",
    "\n",
    "        if self.is_done(old_state):\n",
    "            return []        \n",
    "        \n",
    "        if all_actions:\n",
    "            return [Action.Left, Action.Right, Action.Up, Action.Down]\n",
    "        \n",
    "        possible_actions = []\n",
    "        \n",
    "        # Check whether 'swipe left' is possible or not\n",
    "        state = deepcopy(old_state)\n",
    "        state = self.swipeToLeft(state)\n",
    "        break_out_flag = False\n",
    "        for i in range(self.board_size):\n",
    "            for j in range(self.board_size - 1):\n",
    "                current_tile = state[i][j]\n",
    "                if current_tile != 0:\n",
    "                    right_tile = state[i][j + 1]\n",
    "                    if right_tile == current_tile:\n",
    "                        # left swipe merge is possible\n",
    "                        possible_actions.append(Action.Left)\n",
    "                        \n",
    "                        # exit from nested loop\n",
    "                        break_out_flag = True\n",
    "                        break\n",
    "                        \n",
    "            if break_out_flag:\n",
    "                break\n",
    "                \n",
    "        # Check whether 'swipe right' is possible or not\n",
    "        state = deepcopy(old_state)\n",
    "        state = self.swipeToRight(state)\n",
    "        break_out_flag = False\n",
    "        for i in range(self.board_size):\n",
    "            for j in reversed(range(1, self.board_size)):\n",
    "                current_tile = state[i][j]\n",
    "                if current_tile != 0:\n",
    "                    left_tile = state[i][j - 1]\n",
    "                    if left_tile == current_tile:\n",
    "                        # right swipe merge is possible\n",
    "                        possible_actions.append(Action.Right)\n",
    "                        \n",
    "                        # exit from nested loop\n",
    "                        break_out_flag = True\n",
    "                        break\n",
    "                        \n",
    "            if break_out_flag:\n",
    "                break\n",
    " \n",
    "        # Check whether 'swipe up' is possible or not\n",
    "        state = deepcopy(old_state)\n",
    "        state = self.transpose(state)\n",
    "        state = self.swipeToLeft(state)\n",
    "        break_out_flag = False\n",
    "        for i in range(self.board_size):\n",
    "            for j in range(self.board_size - 1):\n",
    "                current_tile = state[i][j]\n",
    "                if current_tile != 0:\n",
    "                    right_tile = state[i][j + 1]\n",
    "                    if right_tile == current_tile:\n",
    "                        # left swipe merge is possible\n",
    "                        possible_actions.append(Action.Up)\n",
    "                        \n",
    "                        # exit from nested loop\n",
    "                        break_out_flag = True\n",
    "                        break\n",
    "                        \n",
    "            if break_out_flag:\n",
    "                break\n",
    "        state = self.transpose(state)\n",
    " \n",
    "        # Check whether 'swipe down' is possible or not\n",
    "        state = deepcopy(old_state)\n",
    "        state = self.transpose(state)\n",
    "        state = self.swipeToRight(state)\n",
    "        break_out_flag = False\n",
    "        for i in range(self.board_size):\n",
    "            for j in reversed(range(1, self.board_size)):\n",
    "                current_tile = state[i][j]\n",
    "                if current_tile != 0:\n",
    "                    left_tile = state[i][j - 1]\n",
    "                    if left_tile == current_tile:\n",
    "                        # right swipe merge is possible\n",
    "                        possible_actions.append(Action.Down)\n",
    "                        \n",
    "                        # exit from nested loop\n",
    "                        break_out_flag = True\n",
    "                        break\n",
    "                        \n",
    "            if break_out_flag:\n",
    "                break\n",
    "        state = self.transpose(state)\n",
    "        \n",
    "        return possible_actions        \n",
    "        \n",
    "    # determine wheter the game is over\n",
    "    # either: when all cells are occupied and no more merging is possible,\n",
    "    # or 2048 tile is generated\n",
    "    def is_done(self, state:np.ndarray = None):\n",
    "        if state is None:\n",
    "            state = self.__state\n",
    "\n",
    "        # detect if a tile has target value (e.g. 2048)\n",
    "        for i in range(self.board_size):\n",
    "            for j in range(self.board_size):\n",
    "                if self.__state[i][j] == self.target:\n",
    "                    return True\n",
    "\n",
    "        # check if all cells are occupied and no more merging is possible\n",
    "        if 0 not in state:\n",
    "            # no more merging is possible\n",
    "            for i in range(self.board_size - 1):\n",
    "                for j in range(self.board_size - 1):\n",
    "                    if (state[i][j] == state[i + 1][j]) or (\n",
    "                        state[i][j] == state[i][j + 1]\n",
    "                    ):\n",
    "                        return False\n",
    "            # check bottom row\n",
    "            for j in range(self.board_size - 1):\n",
    "                if state[self.board_size - 1][j] == state[self.board_size - 1][j + 1]:\n",
    "                    return False\n",
    "\n",
    "            # check rightmost column\n",
    "            for i in range(self.board_size - 1):\n",
    "                if state[i][self.board_size - 1] == state[i + 1][self.board_size - 1]:\n",
    "                    return False\n",
    "\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def has_won(self, state=None):\n",
    "        if state is None:\n",
    "            state = self.stat\n",
    "    \n",
    "        # detect if a tile has target value (e.g. 2048)\n",
    "        for i in range(self.board_size):\n",
    "            for j in range(self.board_size):\n",
    "                if state[i][j] == self.target:\n",
    "                    return True\n",
    "                    \n",
    "        return False\n",
    "\n",
    "    # Reward R(s) for every possible state [-1,1]\n",
    "    def get_reward(self, state):\n",
    "        # detect tile with target value (e.g. 2048 tile)\n",
    "        for i in range(self.board_size):\n",
    "            for j in range(self.board_size):\n",
    "                if state[i][j] == self.target:\n",
    "                    return 1\n",
    "        \n",
    "        score_reward = 0.0\n",
    "        \n",
    "        # for i in range(self.board_size):\n",
    "        #     for j in range(self.board_size):\n",
    "        #         score_reward += state[i][j]\n",
    "        \n",
    "        score_reward = np.amax(state)/(self.target)\n",
    "        \n",
    "        # check if all cells are occupied and no more merging is possible\n",
    "        if 0 not in state:\n",
    "            # no more merging is possible\n",
    "            for i in range(self.board_size - 1):\n",
    "                for j in range(self.board_size - 1):\n",
    "                    if (state[i][j] == state[i + 1][j]) or (\n",
    "                        state[i][j] == state[i][j + 1]\n",
    "                    ):\n",
    "                        return score_reward\n",
    "            # check bottom row\n",
    "            for j in range(self.board_size - 1):\n",
    "                if state[self.board_size - 1][j] == state[self.board_size - 1][j + 1]:\n",
    "                    return score_reward\n",
    "            # check rightmost column\n",
    "            for i in range(self.board_size - 1):\n",
    "                if state[i][self.board_size - 1] == state[i + 1][self.board_size - 1]:\n",
    "                    return score_reward\n",
    "            return score_reward\n",
    "            \n",
    "        # game is done\n",
    "        return score_reward\n",
    "\n",
    "    def get_transition_prob(self, action, new_state, old_state=None):\n",
    "        if old_state is None:\n",
    "            old_state = self.__state\n",
    "\n",
    "        # if the game is over, no transition can take place\n",
    "        if self.is_done(old_state):\n",
    "            return 0.0\n",
    "\n",
    "        # perform action on old_state\n",
    "        state_after_action = self.__calculate_transition(action, deepcopy(old_state))\n",
    "        # calculate possible states\n",
    "        \n",
    "        possible_states_after_action = self.__calculate_possible_states(deepcopy(old_state), action, depth=0)\n",
    "        # print(f\"possible states (prob): {len(possible_states_after_action)}\")\n",
    "        # print(possible_states_after_action)\n",
    "        \n",
    "        # transition probabilities\n",
    "        prob = 0\n",
    "        if possible_states_after_action is not None:\n",
    "            if not self.contains(possible_states_after_action, new_state):\n",
    "                return 0.0\n",
    "            prob = self.count(possible_states_after_action, new_state) / (len(possible_states_after_action))\n",
    "            # print(f\"probability: {prob}\")\n",
    "        # else:\n",
    "        #     print(f\"None list. {action}\\n {state_after_action}\\n{possible_states_after_action}\")\n",
    "            # self.render(state_after_action)\n",
    "        \n",
    "        return prob\n",
    "        \n",
    "    def contains(self, list, value):\n",
    "        for x in list:\n",
    "            if np.array_equal(x, value):\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    def count(self, list, value):\n",
    "        count = 0\n",
    "        for x in list:\n",
    "            if np.array_equal(x, value):\n",
    "                count += 1\n",
    "        return count    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14644/1454504566.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmdp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGameEnvironment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmdp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_possible_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14644/478669289.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, board_size, target, initial_state, calculate_possible_states)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# during performance assessment, it's not required\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcalculate_possible_states\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__calculate_possible_states_from_different_initial_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__initial_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[1;31m# print(len(self.__possible_states))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14644/478669289.py\u001b[0m in \u001b[0;36m__calculate_possible_states_from_different_initial_states\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     40\u001b[0m                 \u001b[0minitial_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__generate_new_tile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__initial_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__calculate_possible_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14644/478669289.py\u001b[0m in \u001b[0;36m__calculate_possible_states\u001b[1;34m(self, state, action, depth)\u001b[0m\n\u001b[0;32m     77\u001b[0m                     \u001b[0msingleton_possible_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_state\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtile_2_depth\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                         \u001b[1;32mfor\u001b[0m \u001b[0mcalculated_step\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__calculate_possible_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtile_2_depth\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m                             \u001b[0msingleton_possible_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcalculated_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14644/478669289.py\u001b[0m in \u001b[0;36m__calculate_possible_states\u001b[1;34m(self, state, action, depth)\u001b[0m\n\u001b[0;32m     77\u001b[0m                     \u001b[0msingleton_possible_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_done\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_state\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtile_2_depth\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                         \u001b[1;32mfor\u001b[0m \u001b[0mcalculated_step\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__calculate_possible_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdepth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtile_2_depth\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m                             \u001b[0msingleton_possible_states\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcalculated_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14644/478669289.py\u001b[0m in \u001b[0;36m__calculate_possible_states\u001b[1;34m(self, state, action, depth)\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[1;31m# generate new tile at random empty cell\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mtile\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mempty_tiles\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m                     \u001b[0mnew_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutcome_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m                     \u001b[1;31m#if random generated tile is 2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                     \u001b[0mnew_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtile\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[1;34m(x, memo, _nil)\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[0mcopier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__deepcopy__\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m                 \u001b[0mreductor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mdp = GameEnvironment(2, 16)\n",
    "print(len(mdp.get_possible_states()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0, action: Up\n"
     ]
    }
   ],
   "source": [
    "# test tran prob\n",
    "# mdp = GameEnvironment(3, 64, [[0, 2, 2], [0, 0, 0], [0, 0, 2]])\n",
    "mdp = GameEnvironment(2, 16, [[0, 2], [0, 2]])\n",
    "new_state = [[0, 4], [2, 4]]\n",
    "prob = []\n",
    "action = Action.Up\n",
    "Iprob = mdp.get_transition_prob(action, new_state)*100\n",
    "prob.append(Iprob)\n",
    "print(f\"{Iprob}, action: {action}\")\n",
    "    \n",
    "# mdp.render()\n",
    "# mdp.render(new_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state = [[32  2  4]\n",
      " [ 2 16  2]\n",
      " [ 4  8  4]] , reward = 0.5 , done = True\n"
     ]
    }
   ],
   "source": [
    "# example of creation of an environment in the default state\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "def iterativeOutput(flag):\n",
    "    if flag:\n",
    "        time.sleep(1)\n",
    "        clear_output(wait=True)\n",
    "\n",
    "# mdp = GameEnvironment(3, 64, [[0, 2, 2], [0, 0, 0], [0, 0, 0]], calculate_possible_states=False)\n",
    "mdp = GameEnvironment(3, 64, calculate_possible_states=False)\n",
    "# mdp = GameEnvironment(2, 16, calculate_possible_states=False)\n",
    "mdp.reset()\n",
    "# mdp.render()\n",
    "\n",
    "i = 1\n",
    "while not mdp.is_done():\n",
    "    action = randint(1,4) # random choice\n",
    "    state, done, reward, info = mdp.step(Action(action))\n",
    "    print(f\"step {i}) Action taken: {Action(action)}, is done: {done}\")\n",
    "    mdp.render()\n",
    "    i=i+1\n",
    "    iterativeOutput(True)\n",
    "\n",
    "print('state =', state, ', reward =', reward, ', done =', done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print iterations progress\n",
    "def printProgressBar (iteration, total, prefix = 'Progress', suffix = 'Complete', decimals = 1, length = 50, fill = '█', printEnd = \"\\r\"):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start Value iteration for Utility\n",
      "149\n",
      "Created first verison of Utility function-------------------| 0.0% Complete\n",
      "Progress |██████████████████████████████████████████████████| 100.0% Complete\n",
      "value iteration done\n",
      "\n",
      "Progress |██████████████████████████████████████████████████| 100.0% Complete\n",
      "Optimal policy done\n",
      "Optimal Policy:\n",
      "State: [[8 2]\n",
      " [2 0]]; Action: Left\n",
      "State: [[8 2]\n",
      " [2 2]]; Action: Left\n",
      "State: [[8 2]\n",
      " [4 2]]; Action: Up\n",
      "State: [[8 2]\n",
      " [4 4]]; Action: Left\n",
      "State: [[8 4]\n",
      " [2 2]]; Action: Left\n",
      "State: [[8 4]\n",
      " [2 4]]; Action: Up\n",
      "State: [[8 4]\n",
      " [4 4]]; Action: Left\n",
      "State: [[8 4]\n",
      " [2 0]]; Action: Left\n",
      "State: [[8 8]\n",
      " [2 2]]; Action: Left\n",
      "State: [[8 8]\n",
      " [2 4]]; Action: Left\n",
      "State: [[8 0]\n",
      " [2 2]]; Action: Left\n",
      "State: [[8 2]\n",
      " [4 0]]; Action: Left\n",
      "State: [[8 4]\n",
      " [4 0]]; Action: Left\n",
      "State: [[8 0]\n",
      " [4 2]]; Action: Left\n",
      "State: [[4 8]\n",
      " [4 2]]; Action: Up\n",
      "State: [[8 0]\n",
      " [4 4]]; Action: Left\n",
      "State: [[8 2]\n",
      " [8 0]]; Action: Up\n",
      "State: [[8 4]\n",
      " [8 0]]; Action: Up\n",
      "State: [[8 0]\n",
      " [8 2]]; Action: Up\n",
      "State: [[8 0]\n",
      " [8 4]]; Action: Up\n",
      "State: [[2 8]\n",
      " [0 8]]; Action: Up\n",
      "State: [[4 8]\n",
      " [0 8]]; Action: Up\n",
      "State: [[0 8]\n",
      " [2 8]]; Action: Up\n",
      "State: [[0 8]\n",
      " [4 8]]; Action: Up\n",
      "State: [[2 8]\n",
      " [0 4]]; Action: Left\n",
      "State: [[2 8]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 8]\n",
      " [2 4]]; Action: Up\n",
      "State: [[4 8]\n",
      " [0 4]]; Action: Left\n",
      "State: [[4 8]\n",
      " [4 4]]; Action: Left\n",
      "State: [[0 8]\n",
      " [2 4]]; Action: Left\n",
      "State: [[0 8]\n",
      " [4 4]]; Action: Left\n",
      "State: [[8 0]\n",
      " [2 4]]; Action: Left\n",
      "State: [[2 8]\n",
      " [0 2]]; Action: Left\n",
      "State: [[2 8]\n",
      " [2 2]]; Action: Left\n",
      "State: [[4 8]\n",
      " [2 2]]; Action: Left\n",
      "State: [[4 8]\n",
      " [0 2]]; Action: Left\n",
      "State: [[8 8]\n",
      " [4 2]]; Action: Left\n",
      "State: [[0 8]\n",
      " [2 2]]; Action: Left\n",
      "State: [[0 8]\n",
      " [4 2]]; Action: Left\n",
      "State: [[4 4]\n",
      " [2 2]]; Action: Left\n",
      "State: [[4 4]\n",
      " [2 4]]; Action: Left\n",
      "State: [[4 4]\n",
      " [2 8]]; Action: Left\n",
      "State: [[2 8]\n",
      " [2 8]]; Action: Up\n",
      "State: [[4 8]\n",
      " [2 8]]; Action: Left\n",
      "State: [[4 4]\n",
      " [4 2]]; Action: Left\n",
      "State: [[4 4]\n",
      " [8 2]]; Action: Left\n",
      "State: [[8 2]\n",
      " [8 2]]; Action: Up\n",
      "State: [[8 4]\n",
      " [8 2]]; Action: Left\n",
      "State: [[8 2]\n",
      " [8 4]]; Action: Left\n",
      "State: [[8 4]\n",
      " [8 4]]; Action: Left\n",
      "State: [[8 8]\n",
      " [4 4]]; Action: Left\n",
      "State: [[16  2]\n",
      " [ 2  0]]; Action: Left\n",
      "State: [[16  2]\n",
      " [ 4  0]]; Action: Left\n",
      "State: [[16  2]\n",
      " [ 0  2]]; Action: Up\n",
      "State: [[16  2]\n",
      " [ 0  4]]; Action: Left\n",
      "State: [[ 2  0]\n",
      " [16  2]]; Action: Left\n",
      "State: [[ 4  0]\n",
      " [16  2]]; Action: Left\n",
      "State: [[ 0  2]\n",
      " [16  2]]; Action: Up\n",
      "State: [[ 0  4]\n",
      " [16  2]]; Action: Left\n",
      "State: [[16  4]\n",
      " [ 2  0]]; Action: Left\n",
      "State: [[16  4]\n",
      " [ 4  0]]; Action: Left\n",
      "State: [[16  4]\n",
      " [ 0  2]]; Action: Left\n",
      "State: [[16  4]\n",
      " [ 0  4]]; Action: Left\n",
      "State: [[ 2  0]\n",
      " [16  4]]; Action: Left\n",
      "State: [[ 4  0]\n",
      " [16  4]]; Action: Left\n",
      "State: [[ 0  2]\n",
      " [16  4]]; Action: Left\n",
      "State: [[ 0  4]\n",
      " [16  4]]; Action: Left\n",
      "State: [[2 8]\n",
      " [4 8]]; Action: Left\n",
      "State: [[ 2 16]\n",
      " [ 2  0]]; Action: Up\n",
      "State: [[ 2 16]\n",
      " [ 4  0]]; Action: Left\n",
      "State: [[ 2 16]\n",
      " [ 0  2]]; Action: Left\n",
      "State: [[ 2 16]\n",
      " [ 0  4]]; Action: Left\n",
      "State: [[ 2  0]\n",
      " [ 2 16]]; Action: Up\n",
      "State: [[ 4  0]\n",
      " [ 2 16]]; Action: Left\n",
      "State: [[ 0  2]\n",
      " [ 2 16]]; Action: Left\n",
      "State: [[ 0  4]\n",
      " [ 2 16]]; Action: Left\n",
      "State: [[4 8]\n",
      " [4 8]]; Action: Left\n",
      "State: [[ 4 16]\n",
      " [ 2  0]]; Action: Left\n",
      "State: [[ 4 16]\n",
      " [ 4  0]]; Action: Left\n",
      "State: [[ 4 16]\n",
      " [ 0  2]]; Action: Left\n",
      "State: [[ 4 16]\n",
      " [ 0  4]]; Action: Left\n",
      "State: [[ 2  0]\n",
      " [ 4 16]]; Action: Left\n",
      "State: [[ 4  0]\n",
      " [ 4 16]]; Action: Left\n",
      "State: [[ 0  2]\n",
      " [ 4 16]]; Action: Left\n",
      "State: [[ 0  4]\n",
      " [ 4 16]]; Action: Left\n",
      "State: [[4 8]\n",
      " [2 0]]; Action: Left\n",
      "State: [[4 8]\n",
      " [4 0]]; Action: Left\n",
      "State: [[8 8]\n",
      " [2 0]]; Action: Left\n",
      "State: [[16  0]\n",
      " [ 2  2]]; Action: Left\n",
      "State: [[16  0]\n",
      " [ 2  4]]; Action: Left\n",
      "State: [[ 0 16]\n",
      " [ 2  2]]; Action: Left\n",
      "State: [[ 0 16]\n",
      " [ 4  2]]; Action: Left\n",
      "State: [[8 8]\n",
      " [4 0]]; Action: Left\n",
      "State: [[16  0]\n",
      " [ 4  2]]; Action: Left\n",
      "State: [[16  0]\n",
      " [ 4  4]]; Action: Left\n",
      "State: [[ 0 16]\n",
      " [ 2  4]]; Action: Left\n",
      "State: [[ 0 16]\n",
      " [ 4  4]]; Action: Left\n",
      "State: [[8 8]\n",
      " [0 2]]; Action: Left\n",
      "State: [[8 8]\n",
      " [0 4]]; Action: Left\n",
      "State: [[2 0]\n",
      " [8 8]]; Action: Left\n",
      "State: [[ 2  2]\n",
      " [16  0]]; Action: Left\n",
      "State: [[ 2  4]\n",
      " [16  0]]; Action: Left\n",
      "State: [[ 2  2]\n",
      " [ 0 16]]; Action: Left\n",
      "State: [[ 4  2]\n",
      " [ 0 16]]; Action: Left\n",
      "State: [[2 2]\n",
      " [8 8]]; Action: Left\n",
      "State: [[2 4]\n",
      " [8 8]]; Action: Left\n",
      "State: [[4 0]\n",
      " [8 8]]; Action: Left\n",
      "State: [[ 4  2]\n",
      " [16  0]]; Action: Left\n",
      "State: [[ 4  4]\n",
      " [16  0]]; Action: Left\n",
      "State: [[ 2  4]\n",
      " [ 0 16]]; Action: Left\n",
      "State: [[ 4  4]\n",
      " [ 0 16]]; Action: Left\n",
      "State: [[4 2]\n",
      " [8 8]]; Action: Left\n",
      "State: [[4 4]\n",
      " [8 8]]; Action: Left\n",
      "State: [[0 2]\n",
      " [8 8]]; Action: Left\n",
      "State: [[0 4]\n",
      " [8 8]]; Action: Left\n",
      "State: [[2 0]\n",
      " [4 8]]; Action: Left\n",
      "State: [[2 2]\n",
      " [4 8]]; Action: Left\n",
      "State: [[4 2]\n",
      " [4 8]]; Action: Up\n",
      "State: [[4 4]\n",
      " [4 8]]; Action: Left\n",
      "State: [[4 0]\n",
      " [4 8]]; Action: Left\n",
      "State: [[0 2]\n",
      " [4 8]]; Action: Left\n",
      "State: [[0 4]\n",
      " [4 8]]; Action: Left\n",
      "State: [[8 4]\n",
      " [0 2]]; Action: Left\n",
      "State: [[8 4]\n",
      " [0 4]]; Action: Left\n",
      "State: [[2 0]\n",
      " [8 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [8 4]]; Action: Left\n",
      "State: [[4 4]\n",
      " [8 4]]; Action: Left\n",
      "State: [[2 4]\n",
      " [8 4]]; Action: Up\n",
      "State: [[4 0]\n",
      " [8 4]]; Action: Left\n",
      "State: [[0 2]\n",
      " [8 4]]; Action: Left\n",
      "State: [[0 4]\n",
      " [8 4]]; Action: Left\n"
     ]
    }
   ],
   "source": [
    "class Utility:\n",
    "    def __init__(self, state, value):\n",
    "        self.__state = state\n",
    "        self.value = value\n",
    "        \n",
    "    def get_state(self):\n",
    "        return self.__state\n",
    "    \n",
    "    @staticmethod\n",
    "    def findUtilityValue(list, search_state):\n",
    "        for x in list:\n",
    "            if np.array_equal(x.__state, search_state):\n",
    "                # print(x, search_state)\n",
    "                return x.value\n",
    "        return None\n",
    "    @staticmethod\n",
    "    def findUtilityIndex(list, search_state):\n",
    "        index = 0\n",
    "        for x in list:\n",
    "            if np.array_equal(x.__state, search_state):\n",
    "                # print(x.__state, search_state)\n",
    "            \n",
    "                return index\n",
    "            index += 1 \n",
    "        return None\n",
    "\n",
    "def get_initial_U(mdp):\n",
    "    U = []\n",
    "    \n",
    "    for s in possible_states:\n",
    "        U.append(Utility(s, mdp.get_reward(s))) \n",
    "    return U\n",
    "    \n",
    "def Q_Value(mdp, s, a, U):\n",
    "    Q = 0.0\n",
    "    \n",
    "    i = 0\n",
    "    for s_p in possible_states:\n",
    "        P = mdp.get_transition_prob(a, s_p, s)\n",
    "        R = mdp.get_reward(s_p)\n",
    "        utilityValue = Utility.findUtilityValue(U, s_p)\n",
    "        if utilityValue is None:\n",
    "            utilityValue = 0\n",
    "        Q += P * (R + utilityValue)\n",
    "        # print(f\"Q={Q} , step={i}\")\n",
    "        # i+=1\n",
    "    return Q\n",
    "\n",
    "def ValueIteration(mdp, error=0.00001):\n",
    "    printProgressBar(0, len(possible_states))\n",
    "    # from AIMA 4th edition without discount gamma \n",
    "    U_p = get_initial_U(mdp) # U_p = U'\n",
    "    print(\"Created first verison of Utility function\")\n",
    "    delta = float('inf')\n",
    "    # while delta > error:\n",
    "    U = deepcopy(U_p)\n",
    "    \n",
    "    print_U(U)  # to illustrate the iteration process\n",
    "    delta = 0\n",
    "    for step, s in enumerate(possible_states, 1):\n",
    "        max_a = float('-inf')\n",
    "        possible_actions = mdp.get_possible_actions(s)\n",
    "        for a in possible_actions:\n",
    "            q = Q_Value(mdp, s, a, U)\n",
    "            if q > max_a:\n",
    "                max_a = q\n",
    "    \n",
    "        U_p_index = Utility.findUtilityIndex(U_p, s)\n",
    "    \n",
    "        U_p[U_p_index].value = max_a\n",
    "        \n",
    "        U_p_value = U_p[U_p_index].value\n",
    "        U_value = U[Utility.findUtilityIndex(U, s)].value\n",
    "        \n",
    "        if abs(U_p_value - U_value) > delta:\n",
    "            delta = abs(U_p_value - U_value)\n",
    "    #     print(f\"Max 'a': {max_a}\")\n",
    "        # print(f\"delta: {delta}, {U_p_value} - {U_value}\")\n",
    "        printProgressBar(step, len(possible_states))\n",
    "        \n",
    "    # test\n",
    "    # error = -100\n",
    "    return U\n",
    "    \n",
    "def print_U(U):\n",
    "    print('Utilities:')\n",
    "    lastIndex = len(U) - 1\n",
    "    print(U[:5].value)\n",
    "    \n",
    "def print_policy(pi):\n",
    "    print('Optimal Policy:')\n",
    "    for y in pi:\n",
    "        print(f\"State: {y.get_state()}; Action: {y.value}\")\n",
    "\n",
    "mdp = GameEnvironment(2, 16)\n",
    "print(\"start Value iteration for Utility\")\n",
    "possible_states = deepcopy(mdp.get_possible_states())\n",
    "print(len(possible_states))\n",
    "\n",
    "# U(s) = max_a(Q(s,a))\n",
    "U = ValueIteration(mdp)\n",
    "print(\"value iteration done\\n\")\n",
    "# print(U)\n",
    "\n",
    "# pi_star(s) = argmax_a(Q(s,a))\n",
    "pi_star = []\n",
    "for step, s in enumerate(possible_states, 1):\n",
    "    if mdp.is_done(s):\n",
    "        continue # policy is not needed in stop states\n",
    "    max_a = float('-inf')\n",
    "    argmax_a = None\n",
    "    for action in Action:\n",
    "        q = Q_Value(mdp, s, action, U) \n",
    "        if q > max_a:\n",
    "            max_a = q\n",
    "            argmax_a = action\n",
    "    pi_star.append(Utility(s, argmax_a))\n",
    "    printProgressBar(step, len(possible_states))\n",
    "    \n",
    "    \n",
    "print(\"Optimal policy done\")\n",
    "print_policy(pi_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics over 100 episodes\n",
      "ep:   1, has won: 0, total reward:  1.75, random actions took: 0.60\n",
      "ep:   2, has won: 0, total reward:  2.25, random actions took: 0.57\n",
      "ep:   3, has won: 0, total reward:  1.75, random actions took: 0.40\n",
      "ep:   4, has won: 0, total reward: 249.50, random actions took: 0.01\n",
      "ep:   5, has won: 0, total reward:  2.25, random actions took: 0.57\n",
      "ep:   6, has won: 1, total reward:  2.50, random actions took: 0.40\n",
      "ep:   7, has won: 0, total reward:  2.00, random actions took: 0.67\n",
      "ep:   8, has won: 0, total reward:  1.50, random actions took: 0.75\n",
      "ep:   9, has won: 0, total reward:  1.75, random actions took: 0.60\n",
      "ep:  10, has won: 0, total reward:  0.50, random actions took: 0.67\n",
      "ep:  11, has won: 0, total reward: 249.50, random actions took: 0.00\n",
      "ep:  12, has won: 0, total reward:  1.25, random actions took: 0.60\n",
      "ep:  13, has won: 0, total reward: 250.00, random actions took: 0.00\n",
      "ep:  14, has won: 0, total reward: 249.75, random actions took: 0.00\n",
      "ep:  15, has won: 0, total reward:  1.50, random actions took: 0.83\n",
      "ep:  16, has won: 0, total reward:  2.50, random actions took: 0.57\n",
      "ep:  17, has won: 0, total reward:  1.00, random actions took: 0.50\n",
      "ep:  18, has won: 0, total reward:  1.50, random actions took: 0.80\n",
      "ep:  19, has won: 0, total reward: 249.00, random actions took: 0.01\n",
      "ep:  20, has won: 0, total reward:  1.75, random actions took: 0.67\n",
      "ep:  21, has won: 1, total reward:  3.50, random actions took: 0.29\n",
      "ep:  22, has won: 0, total reward:  1.00, random actions took: 0.80\n",
      "ep:  23, has won: 0, total reward:  1.50, random actions took: 0.40\n",
      "ep:  24, has won: 0, total reward:  1.50, random actions took: 0.60\n",
      "ep:  25, has won: 0, total reward:  0.75, random actions took: 0.75\n",
      "ep:  26, has won: 0, total reward:  0.75, random actions took: 0.75\n",
      "ep:  27, has won: 1, total reward:  2.75, random actions took: 0.43\n",
      "ep:  28, has won: 0, total reward:  2.25, random actions took: 0.57\n",
      "ep:  29, has won: 0, total reward:  0.50, random actions took: 0.67\n",
      "ep:  30, has won: 0, total reward: 249.50, random actions took: 0.00\n",
      "ep:  31, has won: 0, total reward: 249.75, random actions took: 0.01\n",
      "ep:  32, has won: 0, total reward:  1.25, random actions took: 0.50\n",
      "ep:  33, has won: 0, total reward:  1.75, random actions took: 0.83\n",
      "ep:  34, has won: 0, total reward:  3.00, random actions took: 0.75\n",
      "ep:  35, has won: 0, total reward:  1.50, random actions took: 0.60\n",
      "ep:  36, has won: 0, total reward:  2.50, random actions took: 0.50\n",
      "ep:  37, has won: 0, total reward:  2.00, random actions took: 0.50\n",
      "ep:  38, has won: 0, total reward:  1.50, random actions took: 0.60\n",
      "ep:  39, has won: 0, total reward: 249.50, random actions took: 0.01\n",
      "ep:  40, has won: 1, total reward:  3.25, random actions took: 0.38\n",
      "ep:  41, has won: 0, total reward:  1.50, random actions took: 0.60\n",
      "ep:  42, has won: 0, total reward:  2.50, random actions took: 0.43\n",
      "ep:  43, has won: 0, total reward:  1.00, random actions took: 0.50\n",
      "ep:  44, has won: 0, total reward: 250.00, random actions took: 0.00\n",
      "ep:  45, has won: 0, total reward:  2.75, random actions took: 0.67\n",
      "ep:  46, has won: 0, total reward:  1.50, random actions took: 0.50\n",
      "ep:  47, has won: 0, total reward: 249.50, random actions took: 0.00\n",
      "ep:  48, has won: 0, total reward:  2.88, random actions took: 0.70\n",
      "ep:  49, has won: 0, total reward:  1.25, random actions took: 0.60\n",
      "ep:  50, has won: 0, total reward:  2.25, random actions took: 0.57\n",
      "ep:  51, has won: 0, total reward:  1.25, random actions took: 0.50\n",
      "ep:  52, has won: 0, total reward:  1.25, random actions took: 0.80\n",
      "ep:  53, has won: 0, total reward:  1.50, random actions took: 0.80\n",
      "ep:  54, has won: 0, total reward:  1.00, random actions took: 0.50\n",
      "ep:  55, has won: 0, total reward:  0.50, random actions took: 0.67\n",
      "ep:  56, has won: 0, total reward: 249.50, random actions took: 0.01\n",
      "ep:  57, has won: 0, total reward:  0.50, random actions took: 0.67\n",
      "ep:  58, has won: 1, total reward:  2.75, random actions took: 0.33\n",
      "ep:  59, has won: 0, total reward: 249.50, random actions took: 0.01\n",
      "ep:  60, has won: 0, total reward:  0.50, random actions took: 0.67\n",
      "ep:  61, has won: 1, total reward:  2.75, random actions took: 0.50\n",
      "ep:  62, has won: 0, total reward:  1.75, random actions took: 0.67\n",
      "ep:  63, has won: 0, total reward:  1.50, random actions took: 0.40\n",
      "ep:  64, has won: 0, total reward:  0.75, random actions took: 0.75\n",
      "ep:  65, has won: 0, total reward:  0.50, random actions took: 0.67\n",
      "ep:  66, has won: 0, total reward:  2.50, random actions took: 0.71\n",
      "ep:  67, has won: 0, total reward:  0.50, random actions took: 0.67\n",
      "ep:  68, has won: 0, total reward:  2.25, random actions took: 0.86\n",
      "ep:  69, has won: 0, total reward:  1.00, random actions took: 0.50\n",
      "ep:  70, has won: 0, total reward:  1.00, random actions took: 0.75\n",
      "ep:  71, has won: 0, total reward:  2.75, random actions took: 0.71\n",
      "ep:  72, has won: 0, total reward:  1.75, random actions took: 0.50\n",
      "ep:  73, has won: 0, total reward:  1.25, random actions took: 0.80\n",
      "ep:  74, has won: 0, total reward:  1.25, random actions took: 0.60\n",
      "ep:  75, has won: 0, total reward:  2.25, random actions took: 0.33\n",
      "ep:  76, has won: 0, total reward:  1.62, random actions took: 0.67\n",
      "ep:  77, has won: 0, total reward:  2.00, random actions took: 0.50\n",
      "ep:  78, has won: 0, total reward:  0.50, random actions took: 0.67\n",
      "ep:  79, has won: 0, total reward:  1.75, random actions took: 0.60\n",
      "ep:  80, has won: 0, total reward:  0.50, random actions took: 0.67\n",
      "ep:  81, has won: 0, total reward:  1.25, random actions took: 0.75\n",
      "ep:  82, has won: 0, total reward:  0.62, random actions took: 0.75\n",
      "ep:  83, has won: 0, total reward:  1.00, random actions took: 0.50\n",
      "ep:  84, has won: 1, total reward:  2.50, random actions took: 0.33\n",
      "ep:  85, has won: 0, total reward: 249.50, random actions took: 0.01\n",
      "ep:  86, has won: 0, total reward:  1.25, random actions took: 0.75\n",
      "ep:  87, has won: 0, total reward:  2.50, random actions took: 0.43\n",
      "ep:  88, has won: 0, total reward:  0.62, random actions took: 0.75\n",
      "ep:  89, has won: 0, total reward:  0.50, random actions took: 0.67\n",
      "ep:  90, has won: 1, total reward:  2.50, random actions took: 0.33\n",
      "ep:  91, has won: 0, total reward:  1.25, random actions took: 0.80\n",
      "ep:  92, has won: 0, total reward:  1.00, random actions took: 0.50\n",
      "ep:  93, has won: 0, total reward:  1.00, random actions took: 0.75\n",
      "ep:  94, has won: 0, total reward:  0.50, random actions took: 0.67\n",
      "ep:  95, has won: 0, total reward:  2.50, random actions took: 0.62\n",
      "ep:  96, has won: 0, total reward:  1.00, random actions took: 0.75\n",
      "ep:  97, has won: 0, total reward:  0.75, random actions took: 0.75\n",
      "ep:  98, has won: 1, total reward:  2.25, random actions took: 0.40\n",
      "ep:  99, has won: 0, total reward:  0.50, random actions took: 0.67\n",
      "ep: 100, has won: 0, total reward: 249.25, random actions took: 0.01\n",
      "win ratio: 9.000%; mean:   0.78, sigma:   0.89, random actions ratio: 52.38%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean, stdev\n",
    "\n",
    "def optimal_policy(state):\n",
    "    return Utility.findUtilityValue(pi_star, state)\n",
    "\n",
    "def run_one_episode(policy, episode, max_steps=-1):\n",
    "    # create game board with 2 random tiles  \n",
    "    mdp = GameEnvironment(2, 16, calculate_possible_states=False)\n",
    "    state = mdp.reset()\n",
    "    # mdp.render()\n",
    "\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    step = 1\n",
    "    random_actions = 0\n",
    "    while not done:\n",
    "        next_action = policy(state)\n",
    "        if next_action == None:\n",
    "            next_action = Action(randint(1,4))\n",
    "            random_actions += 1\n",
    "            \n",
    "        state, done, reward, info = mdp.step(next_action)\n",
    "        # print(state, next_action, done, reward, info)\n",
    "        total_reward += reward\n",
    "        step += 1\n",
    "        if max_steps != -1:\n",
    "            if step > max_steps:\n",
    "                break\n",
    "    \n",
    "    return total_reward, mdp.has_won(mdp.get_state()), random_actions/step\n",
    "\n",
    "def measure_performance(policy, nr_episodes=10):\n",
    "    N = nr_episodes\n",
    "    print('statistics over', N, 'episodes')\n",
    "    all_rewards = []\n",
    "    for n in range(1, N+1):\n",
    "        episode_reward, has_won, random_actions = run_one_episode(policy, n, max_steps=500)\n",
    "        # print('episode:', n, 'reward:', episode_reward)\n",
    "        all_rewards.append([episode_reward, has_won, random_actions])\n",
    "        print('ep: {:3d}, has won: {:1}, total reward: {:5.2f}, random actions took: {:3.2f}'.format(n, has_won, episode_reward, random_actions))\n",
    "    \n",
    "    win_ratio = 0\n",
    "    for x in all_rewards:\n",
    "        if x[1]:\n",
    "            win_ratio += 1\n",
    "    win_ratio = (win_ratio / len(all_rewards)) * 100\n",
    "    \n",
    "    random_actions_ratio = 0\n",
    "    for x in all_rewards:\n",
    "        random_actions_ratio += x[2]\n",
    "    random_actions_ratio = (random_actions_ratio / len(all_rewards)) * 100\n",
    "    \n",
    "    print('win ratio: {:3.3f}%; mean: {:6.2f}, sigma: {:6.2f}, random actions ratio: {:3.2f}%'.format(win_ratio, mean(all_rewards[0]), stdev(all_rewards[0]), random_actions_ratio))\n",
    "    print()\n",
    "    # for n, el in enumerate(all_rewards, 1):\n",
    "    #     print('ep: {:3d}, has won: {:1}, total reward: {:5.2f}, random actions took: {:3.2f}'.format(n, el[1], el[0], el[2]))\n",
    "\n",
    "measure_performance(optimal_policy, nr_episodes = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da3ec9ba63dac011d7c2149d03a658e24415e076227cdc43197121aae5b74ad2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
