{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from random import randint, choice\n",
    "from copy import copy\n",
    "\n",
    "# environment possible actions: swipe to left, right, up, down\n",
    "class Action(Enum):\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "    Left = 1\n",
    "    Right = 2\n",
    "    Up = 3\n",
    "    Down = 4\n",
    "\n",
    "class GameEnvironment():\n",
    "    def __init__(self, board_size=3, target=64, initial_state=None):\n",
    "        if initial_state == None:\n",
    "            # start with empty board\n",
    "            self.__initial_state = [-1 for n in range(board_size*board_size)]\n",
    "        else:\n",
    "            # copy to prevent aliassing\n",
    "            self.__initial_state = copy(initial_state)\n",
    "            \n",
    "        # dynamic board size\n",
    "        self.__board_size = board_size\n",
    "        self.__target = target\n",
    "        self.__state = self.__initial_state\n",
    "        self.__possible_states = []\n",
    "        # maybe to remove\n",
    "        # self.__calculate_possible_states(self.__initial_state)\n",
    "        \n",
    "    # maybe to remove - iterate over all possible states\n",
    "    def __calculate_possible_states(self, state):\n",
    "        actions = self.get_possible_actions(state)\n",
    "        for action in actions:\n",
    "            new_state = copy(state)\n",
    "            if state.count(X) == state.count(O):\n",
    "                new_state[action] = X \n",
    "            else: \n",
    "                new_state[action] = O\n",
    "            self.__possible_states.append(new_state)\n",
    "            if not self.is_done(new_state):\n",
    "                self.__calculate_possible_states(new_state)\n",
    "\n",
    "    def reset(self):\n",
    "        self.__state = self.__initial_state\n",
    "        return self.__state\n",
    "    \n",
    "    # perform action on environment\n",
    "    def __calculate_transition(self, action):\n",
    "        if self.is_done():\n",
    "            return self.__state\n",
    "            \n",
    "        # 1. change the state to reflect the move by the agent\n",
    "        # swipe to left\n",
    "        if action == Action.Left:\n",
    "            self.__state = self.swipeToLeft(self.__state)\n",
    "        # swipe to right\n",
    "        elif action == Action.Right:\n",
    "            self.__state = self.swipeToRight(self.__state)\n",
    "        elif action == Action.Up:\n",
    "            # take transpose\n",
    "            temp_state = self.transpose(self.__state)\n",
    "            temp_state = self.swipeToLeft(temp_state)\n",
    "            self.__state = self.transpose(temp_state)\n",
    "        elif action == Action.Down:\n",
    "             # take transpose\n",
    "            temp_state = self.transpose(self.__state)\n",
    "            temp_state = self.swipeToRight(temp_state)\n",
    "            self.__state = self.transpose(temp_state)\n",
    "        \n",
    "        # 2. merge same value tiles\n",
    "        \n",
    "        \n",
    "        # 3. generate a new tile on empty cells\n",
    "        empty_state = self.get_possible_actions(self.__state)\n",
    "        # possible generated tile values\n",
    "        possible_gen_tiles = [2,4]\n",
    "        # generate new tile at random empty cell\n",
    "        self.__state[choice(empty_state)] = possible_gen_tiles[randint(0,1)]\n",
    "        \n",
    "        return self.__state\n",
    "        \n",
    "    def swipeToLeft(self, state):\n",
    "        for i in range(self.__board_size):\n",
    "            for j in range(self.__board_size-1):\n",
    "                # [0,2,2]: if current cell is empty, swap with the right one \n",
    "                if state[(i*self.__board_size)+j] == -1:\n",
    "                \n",
    "                    # k is the offset of the first found tile    \n",
    "                    for k in range(1, self.__board_size-j):\n",
    "                        if state[(i*self.__board_size)+(j+k)] != -1:\n",
    "                        \n",
    "                            self.swap(state, (i*self.__board_size)+j, (i*self.__board_size)+(j+k))\n",
    "                            break\n",
    "        return state\n",
    "        \n",
    "    def swipeToRight(self, state):\n",
    "        for i in range(self.__board_size):\n",
    "            for j in  reversed(range(1, self.__board_size)):\n",
    "                    # [2,2,0]: if current cell is empty, swap with the left one \n",
    "                    if state[(i*self.__board_size)+j] == -1:\n",
    "                        \n",
    "                        # k is the offset of the first found tile\n",
    "                        for k in range(1, j+1):\n",
    "                            if state[(i*self.__board_size)+(j-k)] != -1:\n",
    "                                \n",
    "                                self.swap(state, (i*self.__board_size)+j, (i*self.__board_size)+(j-k))\n",
    "                                break\n",
    "        return state\n",
    "        \n",
    "    def transpose(self, array):\n",
    "        row, column = self.__board_size, self.__board_size\n",
    "        transposed_array = [-1 for n in range(row*column)]\n",
    "        for i in range(row):\n",
    "            for j in range(column):\n",
    "                transposed_array[(i*row)+j] = array[i+(j*row)]\n",
    "                \n",
    "        return transposed_array\n",
    "    \n",
    "    def swap(self, state, x, y):\n",
    "        z = state[x]\n",
    "        state[x] = state[y]\n",
    "        state[y] = z\n",
    "        \n",
    "    # unit step on environment\n",
    "    def step(self, action):\n",
    "        old_state = self.__state\n",
    "        # state after agent action\n",
    "        self.__state = self.__calculate_transition(action)  \n",
    "        observation = self.__state  # environment is fully observable\n",
    "        done = self.is_done()\n",
    "        reward = self.get_reward(self.__state)\n",
    "        info = {}  # optional debug info\n",
    "        return observation, done, reward, info\n",
    "\n",
    "    # render environment (board) on CLI\n",
    "    def render(self):\n",
    "        print(\"┼───┼────┼───┼\")\n",
    "        \n",
    "        for i in range(self.__board_size):\n",
    "            print(\"│\", end='')\n",
    "            for j in range(self.__board_size):\n",
    "                tile = \"\"\n",
    "                if self.__state[(i*self.__board_size)+j] != -1:\n",
    "                    tile = self.__state[(i*self.__board_size)+j]\n",
    "                    \n",
    "                print(\"{0:>3}\".format(tile) + \"│\", end='')\n",
    "            print()\n",
    "            print(\"┼───┼────┼───┼\")\n",
    "        print()\n",
    "        \n",
    "        \n",
    "    #=========================================================\n",
    "    # public functions for agent to calculate optimal policy\n",
    "    #=========================================================\n",
    "    \n",
    "    def get_possible_states(self):\n",
    "        return self.__possible_states\n",
    "    \n",
    "    # get index of empty cells\n",
    "    def get_possible_actions(self, state=None):\n",
    "        if state is None:\n",
    "            state = self.__state\n",
    "        return [n for n in range(9) if state[n] == -1]\n",
    "\n",
    "    # determine wheter the game is over\n",
    "    # either: when all cells are occupied and no more merging is possible,\n",
    "    # or 2048 tile is generated\n",
    "    def is_done(self, state=None):\n",
    "        if state is None:\n",
    "            state = self.__state\n",
    "            \n",
    "        # detect if a tile has target value (e.g. 2048) \n",
    "        for n in range(self.__board_size*self.__board_size):\n",
    "            if state[n] == self.__target:\n",
    "                return True\n",
    "                \n",
    "        # check if all cells are occupied and no more merging is possible\n",
    "        if -1 not in state:\n",
    "            # no more merging is possible\n",
    "            if True: \n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    # Reward R(s) for every possible state\n",
    "    def get_reward(self, state):\n",
    "        # detect tile with target value (e.g. 2048 tile)\n",
    "        for n in range(self.__board_size*self.__board_size):\n",
    "            if state[n] == self.__target:\n",
    "                return 1\n",
    "        return -1\n",
    "        \n",
    "    # returns the Transition Probability P(s'| s, a)\n",
    "    # def get_transition_prob(self, action, new_state, old_state=None):\n",
    "        # if old_state is None:\n",
    "        #     old_state = self.__state\n",
    "        # # returns the Transition Probability P(s'| s, a)\n",
    "        # # with s = old_state, a = action and s' = new_state\n",
    "\n",
    "        # # if the game is over, no transition can take place\n",
    "        # if self.is_done(old_state):\n",
    "        #     return 0.0\n",
    "        \n",
    "        # # the position of the action must be empty\n",
    "        # if old_state[action] != E:\n",
    "        #     return 0.0\n",
    "        \n",
    "        # # state after placing X\n",
    "        # state_after_X = copy(old_state)  # avoid unwanted changed by reference\n",
    "        # state_after_X[action] = X\n",
    "\n",
    "        # # check if game is done\n",
    "        # if self.is_done(state_after_X) and state_after_X == new_state:\n",
    "        #     return 1.0\n",
    "\n",
    "        # # game is not done: calculate all possible states of the opponent\n",
    "        # possible_new_states = []\n",
    "        # possible_opponent_actions = self.get_possible_actions(state_after_X)\n",
    "        # for action in possible_opponent_actions:\n",
    "        #     possible_new_state = copy(state_after_X)\n",
    "        #     possible_new_state[action] = O\n",
    "        #     possible_new_states.append(possible_new_state)\n",
    "        # if new_state not in possible_new_states:\n",
    "        #     return 0.0\n",
    "        \n",
    "        # # transition is possible, apply strategy:\n",
    "        # # random opponent, probability is 1 / (# of E before placing the new O)\n",
    "        # prob = 1 / (len(possible_new_states))\n",
    "        # return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "┼───┼────┼───┼\n",
      "│    │   2│   2│\n",
      "┼───┼────┼───┼\n",
      "│    │    │    │\n",
      "┼───┼────┼───┼\n",
      "│    │    │    │\n",
      "┼───┼────┼───┼\n",
      "\n",
      "┼───┼────┼───┼\n",
      "│   2│   2│    │\n",
      "┼───┼────┼───┼\n",
      "│    │    │    │\n",
      "┼───┼────┼───┼\n",
      "│   4│    │    │\n",
      "┼───┼────┼───┼\n",
      "\n",
      "┼───┼────┼───┼\n",
      "│    │   2│   2│\n",
      "┼───┼────┼───┼\n",
      "│    │    │    │\n",
      "┼───┼────┼───┼\n",
      "│    │   4│   4│\n",
      "┼───┼────┼───┼\n",
      "\n",
      "UP\n",
      "┼───┼────┼───┼\n",
      "│   2│   2│   2│\n",
      "┼───┼────┼───┼\n",
      "│    │   4│   4│\n",
      "┼───┼────┼───┼\n",
      "│    │    │    │\n",
      "┼───┼────┼───┼\n",
      "\n",
      "Down\n",
      "┼───┼────┼───┼\n",
      "│   2│    │    │\n",
      "┼───┼────┼───┼\n",
      "│    │   2│   2│\n",
      "┼───┼────┼───┼\n",
      "│   2│   4│   4│\n",
      "┼───┼────┼───┼\n",
      "\n",
      "state = [2, -1, -1, -1, 2, 2, 2, 4, 4] , reward = -1 , done = False\n",
      "┼───┼────┼───┼\n",
      "│   2│    │    │\n",
      "┼───┼────┼───┼\n",
      "│    │   2│   2│\n",
      "┼───┼────┼───┼\n",
      "│   2│   4│   4│\n",
      "┼───┼────┼───┼\n",
      "\n",
      "possible (internal) game states:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of creation of an environment in the default state\n",
    "mdp = GameEnvironment(3, 64, [-1, 2, 2, -1, -1, -1, -1, -1, -1])\n",
    "\n",
    "mdp.reset()\n",
    "mdp.render()\n",
    "state, done, reward, info = mdp.step(Action.Left)\n",
    "mdp.render()\n",
    "state, done, reward, info = mdp.step(Action.Right)\n",
    "mdp.render()\n",
    "state, done, reward, info = mdp.step(Action.Up)\n",
    "print(\"UP\")\n",
    "mdp.render()\n",
    "state, done, reward, info = mdp.step(Action.Down)\n",
    "print(\"Down\")\n",
    "mdp.render()\n",
    "print('state =', state, ', reward =', reward, ', done =', done)\n",
    "mdp.render()\n",
    "print('possible (internal) game states:')\n",
    "mdp.get_possible_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = TicTacToeMDPEnvironment(['X', ' ', ' ', \n",
    "                               'O', 'X', ' ', \n",
    "                               ' ', ' ', 'O'])\n",
    "mdp.render()\n",
    "possible_actions = mdp.get_possible_actions()\n",
    "print('possible actions: ', possible_actions)\n",
    "random_agent_action = choice(possible_actions)\n",
    "new_state, done, reward, info = mdp.step(random_agent_action)\n",
    "mdp.render()\n",
    "possible_actions = mdp.get_possible_actions()\n",
    "print('possible actions: ', possible_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_0 = ['X', ' ', ' ', \n",
    "       'O', 'X', ' ', \n",
    "       ' ', ' ', 'O']\n",
    "\n",
    "S_1 = ['X', 'X', 'O', \n",
    "       'O', 'X', ' ', \n",
    "       ' ', ' ', 'O']\n",
    "\n",
    "S_2 = ['X', 'X', ' ', \n",
    "       'O', 'X', 'O', \n",
    "       ' ', ' ', 'O']\n",
    "\n",
    "S_3 = ['X', 'X', ' ', \n",
    "       'O', 'X', ' ', \n",
    "       'O', ' ', 'O']\n",
    "\n",
    "S_4 = ['X', 'X', ' ', \n",
    "       'O', 'X', ' ', \n",
    "       ' ', 'O', 'O']\n",
    "\n",
    "S_5 = ['X', 'X', ' ', \n",
    "       'O', 'X', ' ', \n",
    "       'X', ' ', 'O']\n",
    "\n",
    "mdp = TicTacToeMDPEnvironment(S_0)\n",
    "mdp.render()\n",
    "\n",
    "print('possible actions:', mdp.get_possible_actions())\n",
    "\n",
    "for n, S_p in enumerate([S_1, S_2, S_3, S_4, S_5], 1):\n",
    "    print('S_0 -> action 1 -> S_' + str(n), 'has probability:', mdp.get_transition_prob(1, new_state=S_p))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da3ec9ba63dac011d7c2149d03a658e24415e076227cdc43197121aae5b74ad2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
