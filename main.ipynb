{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"./bean\")\n",
    "from bean.env import GameEnvironment, Action\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of creation of an environment in the default state\n",
    "mdp = GameEnvironment(3, 64, [[0, 2, 2], [0, 0, 0], [0, 0, 0]])\n",
    "mdp.reset()\n",
    "mdp.render()\n",
    "\n",
    "i = 1\n",
    "while not mdp.is_done():\n",
    "    action = randint(1,4) # random choice\n",
    "    state, done, reward, info = mdp.step(Action(action))\n",
    "    print(f\"step {i}) Action taken: {Action(action)}, is done: {done}\")\n",
    "    mdp.render()\n",
    "    i=i+1\n",
    "\n",
    "# state, done, reward, info = mdp.step(Action(2))\n",
    "# mdp.render()\n",
    "# state, done, reward, info = mdp.step(Action.Right)\n",
    "# mdp.render()\n",
    "# state, done, reward, info = mdp.step(Action.Up)\n",
    "# print(\"UP\")\n",
    "# mdp.render()\n",
    "# state, done, reward, info = mdp.step(Action.Down)\n",
    "# print(\"Down\")\n",
    "# mdp.render()\n",
    "\n",
    "print('state =', state, ', reward =', reward, ', done =', done)\n",
    "mdp.render()\n",
    "print('possible (internal) game states:')\n",
    "mdp.get_possible_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = TicTacToeMDPEnvironment(['X', ' ', ' ', \n",
    "                               'O', 'X', ' ', \n",
    "                               ' ', ' ', 'O'])\n",
    "mdp.render()\n",
    "possible_actions = mdp.get_possible_actions()\n",
    "print('possible actions: ', possible_actions)\n",
    "random_agent_action = choice(possible_actions)\n",
    "new_state, done, reward, info = mdp.step(random_agent_action)\n",
    "mdp.render()\n",
    "possible_actions = mdp.get_possible_actions()\n",
    "print('possible actions: ', possible_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_0 = ['X', ' ', ' ', \n",
    "       'O', 'X', ' ', \n",
    "       ' ', ' ', 'O']\n",
    "\n",
    "S_1 = ['X', 'X', 'O', \n",
    "       'O', 'X', ' ', \n",
    "       ' ', ' ', 'O']\n",
    "\n",
    "S_2 = ['X', 'X', ' ', \n",
    "       'O', 'X', 'O', \n",
    "       ' ', ' ', 'O']\n",
    "\n",
    "S_3 = ['X', 'X', ' ', \n",
    "       'O', 'X', ' ', \n",
    "       'O', ' ', 'O']\n",
    "\n",
    "S_4 = ['X', 'X', ' ', \n",
    "       'O', 'X', ' ', \n",
    "       ' ', 'O', 'O']\n",
    "\n",
    "S_5 = ['X', 'X', ' ', \n",
    "       'O', 'X', ' ', \n",
    "       'X', ' ', 'O']\n",
    "\n",
    "mdp = TicTacToeMDPEnvironment(S_0)\n",
    "mdp.render()\n",
    "\n",
    "print('possible actions:', mdp.get_possible_actions())\n",
    "\n",
    "for n, S_p in enumerate([S_1, S_2, S_3, S_4, S_5], 1):\n",
    "    print('S_0 -> action 1 -> S_' + str(n), 'has probability:', mdp.get_transition_prob(1, new_state=S_p))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da3ec9ba63dac011d7c2149d03a658e24415e076227cdc43197121aae5b74ad2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
