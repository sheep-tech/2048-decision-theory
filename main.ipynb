{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tabulate import tabulate  # for rendering board\n",
    "from enum import Enum\n",
    "from random import randint, choice\n",
    "from copy import copy, deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment possible actions: swipe to left, right, up, down\n",
    "class Action(Enum):\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "    Left = 1\n",
    "    Right = 2\n",
    "    Up = 3\n",
    "    Down = 4\n",
    "\n",
    "\n",
    "class GameEnvironment:\n",
    "    def __init__(self, board_size=3, target=64, initial_state=None):\n",
    "        # dynamic board size\n",
    "        self.board_size = board_size\n",
    "        self.won = False\n",
    "        if initial_state == None:\n",
    "            # start with empty board\n",
    "            self.__initial_state = np.zeros([board_size, board_size] ,int)\n",
    "            # generate 2 random tiles on board\n",
    "            for i in range(2):\n",
    "                self.__initial_state = self.__generate_new_tile(self.__initial_state)\n",
    "            \n",
    "        else:\n",
    "            # copy to prevent aliassing\n",
    "            self.__initial_state = copy(initial_state)\n",
    "\n",
    "        self.target = target\n",
    "        self.__state = self.__initial_state\n",
    "        self.__possible_states = []\n",
    "        self.__calculate_possible_states(self.__initial_state, depth=1)\n",
    "        print(len(self.__possible_states))\n",
    "        \n",
    "    # maybe to remove - iterate over all possible states\n",
    "    def __calculate_possible_states(self, state:np.ndarray = None, action=None, depth = 5):\n",
    "        tile_2_depth = copy(depth)\n",
    "        tile_4_depth = copy(depth)\n",
    "        \n",
    "        if state is None:\n",
    "            state = self.__initial_state\n",
    "        \n",
    "        if action == None:\n",
    "            possible_actions = self.get_possible_actions()\n",
    "        else:\n",
    "            possible_actions = [action]\n",
    "            \n",
    "        # possible states returned to get_transition_prob\n",
    "        singleton_possible_states = []\n",
    "            \n",
    "        # get all possible actions \n",
    "        for action in possible_actions:\n",
    "        # calculate the outcome state\n",
    "            outcome_state = self.__calculate_transition(action, state, new_tile=False)\n",
    "        # append to the self.__possible_states (to be used in the utility function)\n",
    "            empty_tiles = self.get_empty_tiles(outcome_state)\n",
    "            if len(empty_tiles) > 0:\n",
    "                # generate new tile at random empty cell\n",
    "                for tile in empty_tiles:\n",
    "                    new_state = deepcopy(outcome_state)   \n",
    "                    #if random generated tile is 2\n",
    "                    new_state[tile[0]][tile[1]] = 2\n",
    "                    temp_state = deepcopy(new_state)\n",
    "                    self.__possible_states.append(temp_state)\n",
    "                    singleton_possible_states.append(temp_state)\n",
    "                    if not self.is_done(temp_state) and tile_2_depth > 0:\n",
    "                        for calculated_step in self.__calculate_possible_states(deepcopy(temp_state),depth = tile_2_depth - 1):\n",
    "                            singleton_possible_states.append(calculated_step)\n",
    "                    #if random generated tile is 4\n",
    "                    new_state[tile[0]][tile[1]] = 4\n",
    "                    temp_state = deepcopy(new_state)\n",
    "                    self.__possible_states.append(temp_state)\n",
    "                    singleton_possible_states.append(temp_state)\n",
    "                    if not self.is_done(temp_state) and tile_4_depth > 0:\n",
    "                        for calculated_step in self.__calculate_possible_states(deepcopy(temp_state),depth = tile_4_depth - 1):\n",
    "                            singleton_possible_states.append(calculated_step)\n",
    "        \n",
    "\n",
    "        # print(f\"possible singleton states: {len(singleton_possible_states)}\")\n",
    "        return singleton_possible_states\n",
    "        \n",
    "    def reset(self):\n",
    "        self.won = False\n",
    "        self.__state = self.__initial_state\n",
    "        return self.__state\n",
    "\n",
    "    # perform action on environment\n",
    "    def __calculate_transition(self, action:Action, state:np.ndarray=None, new_tile:bool = True):        \n",
    "        if state is None:\n",
    "            new_state = self.__state\n",
    "        else:\n",
    "            new_state = deepcopy(state)\n",
    "            \n",
    "        if self.is_done(state):\n",
    "            return new_state\n",
    "\n",
    "        # 1. change the state to reflect the move by the agent,\n",
    "        # 2. merge same value tiles\n",
    "\n",
    "        # swipe to left\n",
    "        if action == Action.Left:\n",
    "            new_state = self.swipeToLeft(new_state)\n",
    "            new_state= self.mergeToLeft(new_state)\n",
    "        # swipe to right\n",
    "        elif action == Action.Right:\n",
    "            new_state = self.swipeToRight(new_state)\n",
    "            new_state = self.mergeToRight(new_state)\n",
    "        elif action == Action.Up:\n",
    "            # take transpose, swipe, then re-take transpose\n",
    "            temp_state = self.transpose(new_state)\n",
    "            temp_state = self.swipeToLeft(temp_state)\n",
    "            temp_state = self.mergeToLeft(temp_state)\n",
    "            new_state = self.transpose(temp_state)\n",
    "        elif action == Action.Down:\n",
    "            # take transpose\n",
    "            temp_state = self.transpose(new_state)\n",
    "            temp_state = self.swipeToRight(temp_state)\n",
    "            temp_state = self.mergeToRight(temp_state)\n",
    "            new_state = self.transpose(temp_state)\n",
    "\n",
    "        # 3. generate a new tile on empty cells\n",
    "        if new_tile:\n",
    "            new_state = self.__generate_new_tile(new_state)\n",
    "            \n",
    "        return new_state\n",
    "\n",
    "    def __generate_new_tile(self, state):\n",
    "        new_state = deepcopy(state)\n",
    "        empty_state = self.get_empty_tiles(new_state)\n",
    "\n",
    "        if len(empty_state) > 0:\n",
    "            # possible generated tile values\n",
    "            possible_gen_tiles = [2, 4]\n",
    "            # generate new tile at random empty cell\n",
    "            row, col = choice(empty_state)\n",
    "            new_state[row][col] = possible_gen_tiles[randint(0, 1)]\n",
    "        return new_state\n",
    "        \n",
    "    def swipeToLeft(self, state):\n",
    "        for i in range(self.board_size):\n",
    "            for j in range(self.board_size - 1):\n",
    "                # [0,2,2]: if current cell is empty, swap with the right one\n",
    "                if state[i][j] == 0:\n",
    "\n",
    "                    # k is the offset of the first found tile\n",
    "                    for k in range(1, self.board_size - j):\n",
    "                        if state[i][j + k] != 0:\n",
    "                            self.swap(state, i, j, i, j + k)\n",
    "                            break\n",
    "        return state\n",
    "\n",
    "    def mergeToLeft(self, state):\n",
    "        # merge same tiles together\n",
    "        for i in range(self.board_size):\n",
    "            for j in range(self.board_size - 1):\n",
    "                current_tile = state[i][j]\n",
    "                if current_tile != 0:\n",
    "                    right_tile = state[i][j + 1]\n",
    "                    if right_tile == current_tile:\n",
    "                        # merge same tiles together\n",
    "                        state[i][j] = current_tile * 2\n",
    "                        state[i][j + 1] = 0\n",
    "                        # shift to the left other tiles\n",
    "                        for k in range(j + 1, self.board_size - 1):\n",
    "                            # current tile equal right tile\n",
    "                            state[i][j + k] = state[i][j + k + 1]\n",
    "\n",
    "                        # last cell is empty\n",
    "                        state[i][self.board_size - 1] = 0\n",
    "\n",
    "        return state\n",
    "\n",
    "    def swipeToRight(self, state):\n",
    "        for i in range(self.board_size):\n",
    "            for j in reversed(range(1, self.board_size)):\n",
    "                # [2,2,0]: if current cell is empty, swap with the left one\n",
    "                if state[i][j] == 0:\n",
    "\n",
    "                    # k is the offset of the first found tile\n",
    "                    for k in range(1, j + 1):\n",
    "                        if state[i][j - k] != 0:\n",
    "\n",
    "                            self.swap(state, i, j, i, j - k)\n",
    "                            break\n",
    "        return state\n",
    "\n",
    "    def mergeToRight(self, state):\n",
    "        # merge same tiles together\n",
    "        for i in range(self.board_size):\n",
    "            for j in reversed(range(1, self.board_size)):\n",
    "                current_tile = state[i][j]\n",
    "                if current_tile != 0:\n",
    "                    left_tile = state[i][j - 1]\n",
    "                    if left_tile == current_tile:\n",
    "                        # merge same tiles together\n",
    "                        state[i][j] = current_tile * 2\n",
    "                        state[i][j - 1] = 0\n",
    "                        # shift to the right other tiles\n",
    "                        for k in reversed(range(1, j - 1)):\n",
    "                            # current tile equal right tile\n",
    "                            state[i][j - k] = state[i][j - k - 1]\n",
    "                        # first cell is empty\n",
    "                        state[i][0] = 0\n",
    "\n",
    "        return state\n",
    "\n",
    "    def transpose(self, array):\n",
    "        transposed_array = np.transpose(array)\n",
    "        return transposed_array\n",
    "\n",
    "    def swap(self, state, x1, y1, x2, y2):\n",
    "        # x and y are the position of the board matrix\n",
    "        z = state[x1][y1]\n",
    "        state[x1][y1] = state[x2][y2]\n",
    "        state[x2][y2] = z\n",
    "\n",
    "    # unit step on environment\n",
    "    def step(self, action):\n",
    "        old_state = self.__state\n",
    "        # state after agent action\n",
    "        self.__state = self.__calculate_transition(action)\n",
    "        observation = self.__state  # environment is fully observable\n",
    "        done = self.is_done()\n",
    "        reward = self.get_reward(self.__state)\n",
    "        info = {}  # optional    debug info\n",
    "        return observation, done, reward, info\n",
    "\n",
    "    # render environment (board) on CLI\n",
    "    def render(self,state:np.ndarray = None):\n",
    "        if state is None:\n",
    "            state = deepcopy(self.__state)\n",
    "        print_state = []\n",
    "        for item in state:\n",
    "            print_state.append(['' if x==0 else x for x in item])\n",
    "        print(tabulate(print_state, tablefmt=\"grid\"))\n",
    "\n",
    "    # =========================================================\n",
    "    # public functions for agent to calculate optimal policy\n",
    "    # =========================================================\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.__state\n",
    "\n",
    "    def get_possible_states(self):\n",
    "        return self.__possible_states\n",
    "\n",
    "    # get index of empty cells\n",
    "    def get_empty_tiles(self, state=None):\n",
    "        if state is None:\n",
    "            state = self.__state\n",
    "        empty_cells = []\n",
    "        for i in range(self.board_size):\n",
    "            for j in range(self.board_size):\n",
    "                if state[i][j] == 0:\n",
    "                    empty_cells.append([i, j])\n",
    "\n",
    "        return empty_cells\n",
    "\n",
    "    def get_possible_actions(self, old_state:np.ndarray = None):\n",
    "        if old_state is None:\n",
    "            old_state = copy(self.__initial_state)\n",
    "\n",
    "        if self.is_done(old_state):\n",
    "            return []        \n",
    "        \n",
    "        return [Action.Left, Action.Right, Action.Up, Action.Down]\n",
    "        \n",
    "        possible_actions = []\n",
    "        \n",
    "        # Check whether 'swipe left' is possible or not\n",
    "        state = deepcopy(old_state)\n",
    "        state = self.swipeToLeft(state)\n",
    "        break_out_flag = False\n",
    "        for i in range(self.board_size):\n",
    "            for j in range(self.board_size - 1):\n",
    "                current_tile = state[i][j]\n",
    "                if current_tile != 0:\n",
    "                    right_tile = state[i][j + 1]\n",
    "                    if right_tile == current_tile:\n",
    "                        # left swipe merge is possible\n",
    "                        possible_actions.append(Action.Left)\n",
    "                        \n",
    "                        # exit from nested loop\n",
    "                        break_out_flag = True\n",
    "                        break\n",
    "                        \n",
    "            if break_out_flag:\n",
    "                break\n",
    "                \n",
    "        # Check whether 'swipe right' is possible or not\n",
    "        state = deepcopy(old_state)\n",
    "        state = self.swipeToRight(state)\n",
    "        break_out_flag = False\n",
    "        for i in range(self.board_size):\n",
    "            for j in reversed(range(1, self.board_size)):\n",
    "                current_tile = state[i][j]\n",
    "                if current_tile != 0:\n",
    "                    left_tile = state[i][j - 1]\n",
    "                    if left_tile == current_tile:\n",
    "                        # right swipe merge is possible\n",
    "                        possible_actions.append(Action.Right)\n",
    "                        \n",
    "                        # exit from nested loop\n",
    "                        break_out_flag = True\n",
    "                        break\n",
    "                        \n",
    "            if break_out_flag:\n",
    "                break\n",
    " \n",
    "        # Check whether 'swipe up' is possible or not\n",
    "        state = deepcopy(old_state)\n",
    "        state = self.transpose(state)\n",
    "        state = self.swipeToLeft(state)\n",
    "        break_out_flag = False\n",
    "        for i in range(self.board_size):\n",
    "            for j in range(self.board_size - 1):\n",
    "                current_tile = state[i][j]\n",
    "                if current_tile != 0:\n",
    "                    right_tile = state[i][j + 1]\n",
    "                    if right_tile == current_tile:\n",
    "                        # left swipe merge is possible\n",
    "                        possible_actions.append(Action.Up)\n",
    "                        \n",
    "                        # exit from nested loop\n",
    "                        break_out_flag = True\n",
    "                        break\n",
    "                        \n",
    "            if break_out_flag:\n",
    "                break\n",
    "        state = self.transpose(state)\n",
    " \n",
    "        # Check whether 'swipe down' is possible or not\n",
    "        state = deepcopy(old_state)\n",
    "        state = self.transpose(state)\n",
    "        state = self.swipeToRight(state)\n",
    "        break_out_flag = False\n",
    "        for i in range(self.board_size):\n",
    "            for j in reversed(range(1, self.board_size)):\n",
    "                current_tile = state[i][j]\n",
    "                if current_tile != 0:\n",
    "                    left_tile = state[i][j - 1]\n",
    "                    if left_tile == current_tile:\n",
    "                        # right swipe merge is possible\n",
    "                        possible_actions.append(Action.Down)\n",
    "                        \n",
    "                        # exit from nested loop\n",
    "                        break_out_flag = True\n",
    "                        break\n",
    "                        \n",
    "            if break_out_flag:\n",
    "                break\n",
    "        state = self.transpose(state)\n",
    "        \n",
    "        return possible_actions        \n",
    "        \n",
    "    # determine wheter the game is over\n",
    "    # either: when all cells are occupied and no more merging is possible,\n",
    "    # or 2048 tile is generated\n",
    "    def is_done(self, state:np.ndarray = None):\n",
    "        if state is None:\n",
    "            state = self.__state\n",
    "\n",
    "        # detect if a tile has target value (e.g. 2048)\n",
    "        for i in range(self.board_size):\n",
    "            for j in range(self.board_size):\n",
    "                if self.__state[i][j] == self.target:\n",
    "                    return True\n",
    "\n",
    "        # check if all cells are occupied and no more merging is possible\n",
    "        if 0 not in state:\n",
    "            # no more merging is possible\n",
    "            for i in range(self.board_size - 1):\n",
    "                for j in range(self.board_size - 1):\n",
    "                    if (state[i][j] == state[i + 1][j]) or (\n",
    "                        state[i][j] == state[i][j + 1]\n",
    "                    ):\n",
    "                        return False\n",
    "            # check bottom row\n",
    "            for j in range(self.board_size - 1):\n",
    "                if state[self.board_size - 1][j] == state[self.board_size - 1][j + 1]:\n",
    "                    return False\n",
    "\n",
    "            # check rightmost column\n",
    "            for i in range(self.board_size - 1):\n",
    "                if state[i][self.board_size - 1] == state[i + 1][self.board_size - 1]:\n",
    "                    return False\n",
    "\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def has_won(self, state=None):\n",
    "        if state is None:\n",
    "            state = self.stat\n",
    "    \n",
    "        # detect if a tile has target value (e.g. 2048)\n",
    "        for i in range(self.board_size):\n",
    "            for j in range(self.board_size):\n",
    "                if state[i][j] == self.target:\n",
    "                    return True\n",
    "                    \n",
    "        return False\n",
    "\n",
    "    # Reward R(s) for every possible state [-1,1]\n",
    "    def get_reward(self, state):\n",
    "        # detect tile with target value (e.g. 2048 tile)\n",
    "        for i in range(self.board_size):\n",
    "            for j in range(self.board_size):\n",
    "                if state[i][j] == self.target:\n",
    "                    return 1\n",
    "        \n",
    "        score_reward = 0.0\n",
    "        \n",
    "        # for i in range(self.board_size):\n",
    "        #     for j in range(self.board_size):\n",
    "        #         score_reward += state[i][j]\n",
    "        \n",
    "        score_reward = np.amax(state)/(self.target)\n",
    "        \n",
    "        # check if all cells are occupied and no more merging is possible\n",
    "        if 0 not in state:\n",
    "            # no more merging is possible\n",
    "            for i in range(self.board_size - 1):\n",
    "                for j in range(self.board_size - 1):\n",
    "                    if (state[i][j] == state[i + 1][j]) or (\n",
    "                        state[i][j] == state[i][j + 1]\n",
    "                    ):\n",
    "                        return score_reward\n",
    "            # check bottom row\n",
    "            for j in range(self.board_size - 1):\n",
    "                if state[self.board_size - 1][j] == state[self.board_size - 1][j + 1]:\n",
    "                    return score_reward\n",
    "            # check rightmost column\n",
    "            for i in range(self.board_size - 1):\n",
    "                if state[i][self.board_size - 1] == state[i + 1][self.board_size - 1]:\n",
    "                    return score_reward\n",
    "            return score_reward\n",
    "            \n",
    "        # game is done\n",
    "        return -1\n",
    "\n",
    "    def get_transition_prob(self, action, new_state, old_state=None):\n",
    "        if old_state is None:\n",
    "            old_state = self.__state\n",
    "\n",
    "        # if the game is over, no transition can take place\n",
    "        if self.is_done(old_state):\n",
    "            return 0.0\n",
    "\n",
    "        # perform action on old_state\n",
    "        state_after_action = self.__calculate_transition(action, deepcopy(old_state))\n",
    "        # calculate possible states\n",
    "        \n",
    "        possible_states_after_action = self.__calculate_possible_states(deepcopy(old_state), action, depth=0)\n",
    "        # print(f\"possible states (prob): {len(possible_states_after_action)}\")\n",
    "        # print(possible_states_after_action)\n",
    "        \n",
    "        # transition probabilities\n",
    "        prob = 0\n",
    "        if possible_states_after_action is not None:\n",
    "            if not self.contains(possible_states_after_action, new_state):\n",
    "                return 0.0\n",
    "            prob = self.count(possible_states_after_action, new_state) / (len(possible_states_after_action))\n",
    "            # print(f\"probability: {prob}\")\n",
    "        # else:\n",
    "        #     print(f\"None list. {action}\\n {state_after_action}\\n{possible_states_after_action}\")\n",
    "            # self.render(state_after_action)\n",
    "        \n",
    "        return prob\n",
    "        \n",
    "    def contains(self, list, value):\n",
    "        for x in list:\n",
    "            if np.array_equal(x, value):\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    def count(self, list, value):\n",
    "        count = 0\n",
    "        for x in list:\n",
    "            if np.array_equal(x, value):\n",
    "                count += 1\n",
    "        return count    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0, action: Up\n"
     ]
    }
   ],
   "source": [
    "# test tran prob\n",
    "# mdp = GameEnvironment(3, 64, [[0, 2, 2], [0, 0, 0], [0, 0, 2]])\n",
    "mdp = GameEnvironment(2, 16, [[0, 2], [0, 2]])\n",
    "new_state = [[0, 4], [2, 4]]\n",
    "prob = []\n",
    "action = Action.Up\n",
    "Iprob = mdp.get_transition_prob(action, new_state)*100\n",
    "prob.append(Iprob)\n",
    "print(f\"{Iprob}, action: {action}\")\n",
    "    \n",
    "# mdp.render()\n",
    "# mdp.render(new_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1) Action taken: Up, is done: False\n",
      "+---+---+\n",
      "| 2 | 2 |\n",
      "+---+---+\n",
      "|   | 2 |\n",
      "+---+---+\n",
      "step 2) Action taken: Left, is done: False\n",
      "+---+---+\n",
      "| 4 |   |\n",
      "+---+---+\n",
      "| 2 | 2 |\n",
      "+---+---+\n",
      "step 3) Action taken: Down, is done: False\n",
      "+---+---+\n",
      "| 4 | 4 |\n",
      "+---+---+\n",
      "| 2 | 2 |\n",
      "+---+---+\n",
      "step 4) Action taken: Right, is done: False\n",
      "+---+---+\n",
      "| 2 | 8 |\n",
      "+---+---+\n",
      "|   | 4 |\n",
      "+---+---+\n",
      "step 5) Action taken: Left, is done: False\n",
      "+---+---+\n",
      "| 2 | 8 |\n",
      "+---+---+\n",
      "| 4 | 4 |\n",
      "+---+---+\n",
      "step 6) Action taken: Down, is done: False\n",
      "+---+---+\n",
      "| 2 | 8 |\n",
      "+---+---+\n",
      "| 4 | 4 |\n",
      "+---+---+\n",
      "step 7) Action taken: Right, is done: False\n",
      "+---+---+\n",
      "| 2 | 8 |\n",
      "+---+---+\n",
      "| 2 | 8 |\n",
      "+---+---+\n",
      "step 8) Action taken: Right, is done: False\n",
      "+---+---+\n",
      "| 2 | 8 |\n",
      "+---+---+\n",
      "| 2 | 8 |\n",
      "+---+---+\n",
      "step 9) Action taken: Right, is done: False\n",
      "+---+---+\n",
      "| 2 | 8 |\n",
      "+---+---+\n",
      "| 2 | 8 |\n",
      "+---+---+\n",
      "step 10) Action taken: Left, is done: False\n",
      "+---+---+\n",
      "| 2 | 8 |\n",
      "+---+---+\n",
      "| 2 | 8 |\n",
      "+---+---+\n",
      "step 11) Action taken: Right, is done: False\n",
      "+---+---+\n",
      "| 2 | 8 |\n",
      "+---+---+\n",
      "| 2 | 8 |\n",
      "+---+---+\n",
      "step 12) Action taken: Left, is done: False\n",
      "+---+---+\n",
      "| 2 | 8 |\n",
      "+---+---+\n",
      "| 2 | 8 |\n",
      "+---+---+\n",
      "step 13) Action taken: Right, is done: False\n",
      "+---+---+\n",
      "| 2 | 8 |\n",
      "+---+---+\n",
      "| 2 | 8 |\n",
      "+---+---+\n",
      "step 14) Action taken: Right, is done: False\n",
      "+---+---+\n",
      "| 2 | 8 |\n",
      "+---+---+\n",
      "| 2 | 8 |\n",
      "+---+---+\n",
      "step 15) Action taken: Down, is done: True\n",
      "+---+----+\n",
      "|   |  2 |\n",
      "+---+----+\n",
      "| 4 | 16 |\n",
      "+---+----+\n",
      "state = [[ 0  2]\n",
      " [ 4 16]] , reward = 1 , done = True\n"
     ]
    }
   ],
   "source": [
    "# example of creation of an environment in the default state\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "def iterativeOutput(flag):\n",
    "    if flag:\n",
    "        time.sleep(1)\n",
    "        clear_output(wait=True)\n",
    "\n",
    "# mdp = GameEnvironment(3, 64, [[0, 2, 2], [0, 0, 0], [0, 0, 0]])\n",
    "mdp = GameEnvironment(2, 16, [[2, 2], [0, 0]])\n",
    "mdp.reset()\n",
    "# mdp.render()\n",
    "\n",
    "i = 1\n",
    "while not mdp.is_done():\n",
    "    action = randint(1,4) # random choice\n",
    "    state, done, reward, info = mdp.step(Action(action))\n",
    "    print(f\"step {i}) Action taken: {Action(action)}, is done: {done}\")\n",
    "    mdp.render()\n",
    "    i=i+1\n",
    "    iterativeOutput(False)\n",
    "\n",
    "print('state =', state, ', reward =', reward, ', done =', done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print iterations progress\n",
    "def printProgressBar (iteration, total, prefix = 'Progress', suffix = 'Complete', decimals = 1, length = 50, fill = '█', printEnd = \"\\r\"):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end = printEnd)\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total: \n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start Value iteration for Utility\n",
      "176---------------------------------------------------------------------------------------------------| 0.0% \n",
      " |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% \n",
      "value iteration done\n",
      "Optimal policy done\n",
      "Optimal Policy:\n",
      "State: [[2 2]\n",
      " [4 0]]; Action: Down\n",
      "State: [[4 2]\n",
      " [4 0]]; Action: Right\n",
      "State: [[4 4]\n",
      " [4 0]]; Action: Left\n",
      "State: [[4 0]\n",
      " [4 2]]; Action: Right\n",
      "State: [[4 0]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 4]\n",
      " [0 4]]; Action: Left\n",
      "State: [[4 4]\n",
      " [0 4]]; Action: Left\n",
      "State: [[0 4]\n",
      " [2 4]]; Action: Left\n",
      "State: [[0 4]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [4 2]]; Action: Up\n",
      "State: [[2 2]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [4 2]]; Action: Up\n",
      "State: [[2 4]\n",
      " [4 0]]; Action: Left\n",
      "State: [[2 4]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 4]\n",
      " [2 4]]; Action: Left\n",
      "State: [[2 4]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 4]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 4]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 0]\n",
      " [4 2]]; Action: Left\n",
      "State: [[2 2]\n",
      " [4 2]]; Action: Up\n",
      "State: [[2 2]\n",
      " [4 2]]; Action: Up\n",
      "State: [[4 2]\n",
      " [4 2]]; Action: Left\n",
      "State: [[2 2]\n",
      " [4 2]]; Action: Up\n",
      "State: [[2 2]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [4 2]]; Action: Up\n",
      "State: [[2 0]\n",
      " [4 4]]; Action: Up\n",
      "State: [[2 2]\n",
      " [8 0]]; Action: Left\n",
      "State: [[2 4]\n",
      " [8 0]]; Action: Left\n",
      "State: [[2 0]\n",
      " [8 2]]; Action: Left\n",
      "State: [[2 0]\n",
      " [8 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [0 8]]; Action: Left\n",
      "State: [[4 2]\n",
      " [0 8]]; Action: Left\n",
      "State: [[0 2]\n",
      " [2 8]]; Action: Left\n",
      "State: [[0 2]\n",
      " [4 8]]; Action: Left\n",
      "State: [[2 4]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 4]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [0 4]]; Action: Up\n",
      "State: [[4 2]\n",
      " [4 0]]; Action: Right\n",
      "State: [[4 4]\n",
      " [4 0]]; Action: Left\n",
      "State: [[4 0]\n",
      " [4 2]]; Action: Right\n",
      "State: [[4 0]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 4]\n",
      " [0 4]]; Action: Left\n",
      "State: [[4 4]\n",
      " [0 4]]; Action: Left\n",
      "State: [[0 4]\n",
      " [2 4]]; Action: Left\n",
      "State: [[0 4]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [2 4]]; Action: Down\n",
      "State: [[2 2]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [2 4]]; Action: Down\n",
      "State: [[4 2]\n",
      " [0 4]]; Action: Down\n",
      "State: [[4 2]\n",
      " [4 2]]; Action: Left\n",
      "State: [[4 2]\n",
      " [4 4]]; Action: Left\n",
      "State: [[4 2]\n",
      " [4 4]]; Action: Left\n",
      "State: [[4 2]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [4 4]]; Action: Left\n",
      "State: [[4 2]\n",
      " [4 4]]; Action: Left\n",
      "State: [[0 2]\n",
      " [2 4]]; Action: Up\n",
      "State: [[2 2]\n",
      " [2 4]]; Action: Down\n",
      "State: [[2 4]\n",
      " [2 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [2 4]]; Action: Down\n",
      "State: [[2 2]\n",
      " [2 4]]; Action: Down\n",
      "State: [[2 2]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [2 4]]; Action: Down\n",
      "State: [[0 2]\n",
      " [4 4]]; Action: Down\n",
      "State: [[2 2]\n",
      " [8 0]]; Action: Left\n",
      "State: [[2 4]\n",
      " [8 0]]; Action: Left\n",
      "State: [[2 0]\n",
      " [8 2]]; Action: Left\n",
      "State: [[2 0]\n",
      " [8 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [0 8]]; Action: Left\n",
      "State: [[4 2]\n",
      " [0 8]]; Action: Left\n",
      "State: [[0 2]\n",
      " [2 8]]; Action: Left\n",
      "State: [[0 2]\n",
      " [4 8]]; Action: Left\n",
      "State: [[4 2]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [4 4]]; Action: Left\n",
      "State: [[4 2]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [4 0]]; Action: Down\n",
      "State: [[4 2]\n",
      " [4 0]]; Action: Right\n",
      "State: [[4 4]\n",
      " [4 0]]; Action: Left\n",
      "State: [[4 0]\n",
      " [4 2]]; Action: Right\n",
      "State: [[4 0]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 4]\n",
      " [0 4]]; Action: Left\n",
      "State: [[4 4]\n",
      " [0 4]]; Action: Left\n",
      "State: [[0 4]\n",
      " [2 4]]; Action: Left\n",
      "State: [[0 4]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [4 2]]; Action: Up\n",
      "State: [[2 2]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [4 2]]; Action: Up\n",
      "State: [[2 4]\n",
      " [4 0]]; Action: Left\n",
      "State: [[2 4]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 4]\n",
      " [2 4]]; Action: Left\n",
      "State: [[2 4]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 4]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 4]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 0]\n",
      " [4 2]]; Action: Left\n",
      "State: [[2 2]\n",
      " [4 2]]; Action: Up\n",
      "State: [[2 2]\n",
      " [4 2]]; Action: Up\n",
      "State: [[4 2]\n",
      " [4 2]]; Action: Left\n",
      "State: [[2 2]\n",
      " [4 2]]; Action: Up\n",
      "State: [[2 2]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [4 2]]; Action: Up\n",
      "State: [[2 0]\n",
      " [4 4]]; Action: Up\n",
      "State: [[2 2]\n",
      " [8 0]]; Action: Left\n",
      "State: [[2 4]\n",
      " [8 0]]; Action: Left\n",
      "State: [[2 0]\n",
      " [8 2]]; Action: Left\n",
      "State: [[2 0]\n",
      " [8 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [0 8]]; Action: Left\n",
      "State: [[4 2]\n",
      " [0 8]]; Action: Left\n",
      "State: [[0 2]\n",
      " [2 8]]; Action: Left\n",
      "State: [[0 2]\n",
      " [4 8]]; Action: Left\n",
      "State: [[2 4]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 4]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [4 0]]; Action: Down\n",
      "State: [[4 2]\n",
      " [4 0]]; Action: Right\n",
      "State: [[4 4]\n",
      " [4 0]]; Action: Left\n",
      "State: [[4 0]\n",
      " [4 2]]; Action: Right\n",
      "State: [[4 0]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 4]\n",
      " [0 4]]; Action: Left\n",
      "State: [[4 4]\n",
      " [0 4]]; Action: Left\n",
      "State: [[0 4]\n",
      " [2 4]]; Action: Left\n",
      "State: [[0 4]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [4 2]]; Action: Up\n",
      "State: [[2 2]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [4 2]]; Action: Up\n",
      "State: [[2 4]\n",
      " [4 0]]; Action: Left\n",
      "State: [[2 4]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 4]\n",
      " [2 4]]; Action: Left\n",
      "State: [[2 4]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 4]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 4]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 0]\n",
      " [4 2]]; Action: Left\n",
      "State: [[2 2]\n",
      " [4 2]]; Action: Up\n",
      "State: [[2 2]\n",
      " [4 2]]; Action: Up\n",
      "State: [[4 2]\n",
      " [4 2]]; Action: Left\n",
      "State: [[2 2]\n",
      " [4 2]]; Action: Up\n",
      "State: [[2 2]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [4 2]]; Action: Up\n",
      "State: [[2 0]\n",
      " [4 4]]; Action: Up\n",
      "State: [[2 2]\n",
      " [8 0]]; Action: Left\n",
      "State: [[2 4]\n",
      " [8 0]]; Action: Left\n",
      "State: [[2 0]\n",
      " [8 2]]; Action: Left\n",
      "State: [[2 0]\n",
      " [8 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [0 8]]; Action: Left\n",
      "State: [[4 2]\n",
      " [0 8]]; Action: Left\n",
      "State: [[0 2]\n",
      " [2 8]]; Action: Left\n",
      "State: [[0 2]\n",
      " [4 8]]; Action: Left\n",
      "State: [[2 4]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 2]\n",
      " [4 4]]; Action: Left\n",
      "State: [[2 4]\n",
      " [4 4]]; Action: Left\n"
     ]
    }
   ],
   "source": [
    "class Utility:\n",
    "    def __init__(self, state, value):\n",
    "        self.__state = state\n",
    "        self.value = value\n",
    "        \n",
    "    def get_state(self):\n",
    "        return self.__state\n",
    "    \n",
    "    @staticmethod\n",
    "    def findUtilityValue(list, search_state):\n",
    "        for x in list:\n",
    "            if np.array_equal(x, search_state):\n",
    "                # print(x, search_state)\n",
    "                return x.value\n",
    "        return None\n",
    "    @staticmethod\n",
    "    def findUtilityIndex(list, search_state):\n",
    "        index = 0\n",
    "        for x in list:\n",
    "            if np.array_equal(x.__state, search_state):\n",
    "                # print(x.__state, search_state)\n",
    "            \n",
    "                return index\n",
    "            index += 1 \n",
    "        return None\n",
    "\n",
    "def get_initial_U(mdp):\n",
    "    U = []\n",
    "    \n",
    "    print(len(possible_states))\n",
    "    for s in possible_states:\n",
    "        U.append(Utility(s, mdp.get_reward(s))) \n",
    "    return U\n",
    "    \n",
    "def Q_Value(mdp, s, a, U):\n",
    "    Q = 0.0\n",
    "    \n",
    "    i = 0\n",
    "    for s_p in possible_states:\n",
    "        P = mdp.get_transition_prob(a, s_p, s)\n",
    "        R = mdp.get_reward(s_p)\n",
    "        utilityValue = Utility.findUtilityValue(U, s_p)\n",
    "        if utilityValue is None:\n",
    "            utilityValue = 0\n",
    "        Q += P * (R + utilityValue)\n",
    "        # print(f\"Q={Q} , step={i}\")\n",
    "        # i+=1\n",
    "    return Q\n",
    "\n",
    "def ValueIteration(mdp, error=0.00001):\n",
    "    printProgressBar(0, len(possible_states))\n",
    "    # from AIMA 4th edition without discount gamma \n",
    "    U_p = get_initial_U(mdp) # U_p = U'\n",
    "    delta = float('inf')\n",
    "    # while delta > error:\n",
    "    U = deepcopy(U_p)\n",
    "    \n",
    "    # print_U(U)  # to illustrate the iteration process\n",
    "    delta = 0\n",
    "    for step, s in enumerate(possible_states, 1):\n",
    "        max_a = float('-inf')\n",
    "        for a in mdp.get_possible_actions(s):\n",
    "            q = Q_Value(mdp, s, a, U) \n",
    "            if q > max_a:\n",
    "                max_a = q\n",
    "    \n",
    "        U_p_index = Utility.findUtilityIndex(U_p, s)\n",
    "    \n",
    "        U_p[U_p_index].value = max_a\n",
    "        \n",
    "        U_p_value = U_p[U_p_index].value\n",
    "        U_value = U[Utility.findUtilityIndex(U, s)].value\n",
    "        \n",
    "        if abs(U_p_value - U_value) > delta:\n",
    "            delta = abs(U_p_value - U_value)\n",
    "    #     print(f\"Max 'a': {max_a}\")\n",
    "        # print(f\"delta: {delta}, {U_p_value} - {U_value}\")\n",
    "        printProgressBar(step, len(possible_states))\n",
    "        \n",
    "    # test\n",
    "    # error = -100\n",
    "    return U\n",
    "    \n",
    "def print_U(U):\n",
    "    print('Utilities:')\n",
    "    lastIndex = len(U) - 1\n",
    "    print(U[lastIndex].value)\n",
    "    \n",
    "def print_policy(pi):\n",
    "    print('Optimal Policy:')\n",
    "    for y in pi:\n",
    "        print(f\"State: {y.get_state()}; Action: {y.value}\")\n",
    "\n",
    "mdp = GameEnvironment(2, 16)\n",
    "print(\"start Value iteration for Utility\")\n",
    "possible_states = deepcopy(mdp.get_possible_states())\n",
    "\n",
    "# U(s) = max_a(Q(s,a))\n",
    "U = ValueIteration(mdp)\n",
    "print(\"value iteration done\")\n",
    "# print(U)\n",
    "\n",
    "# pi_star(s) = argmax_a(Q(s,a))\n",
    "pi_star = []\n",
    "for s in possible_states:\n",
    "    if mdp.is_done(s):\n",
    "        continue # policy is not needed in stop states\n",
    "    max_a = float('-inf')\n",
    "    argmax_a = None\n",
    "    for action in Action:\n",
    "        q = Q_Value(mdp, s, action, U) \n",
    "        if q > max_a:\n",
    "            max_a = q\n",
    "            argmax_a = action\n",
    "    pi_star.append(Utility(s, argmax_a))\n",
    "\n",
    "print(\"Optimal policy done\")\n",
    "print_policy(pi_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics over 100 episodes\n",
      "176\n",
      "176\n",
      "176\n",
      "176\n",
      "340\n",
      "176\n",
      "324\n",
      "240\n",
      "176\n",
      "324\n",
      "176\n",
      "176\n",
      "176\n",
      "240\n",
      "324\n",
      "324\n",
      "176\n",
      "340\n",
      "340\n",
      "176\n",
      "176\n",
      "340\n",
      "176\n",
      "176\n",
      "176\n",
      "176\n",
      "240\n",
      "176\n",
      "176\n",
      "176\n",
      "324\n",
      "176\n",
      "240\n",
      "176\n",
      "176\n",
      "176\n",
      "240\n",
      "324\n",
      "340\n",
      "340\n",
      "176\n",
      "324\n",
      "176\n",
      "340\n",
      "324\n",
      "340\n",
      "176\n",
      "340\n",
      "176\n",
      "176\n",
      "176\n",
      "324\n",
      "240\n",
      "340\n",
      "176\n",
      "240\n",
      "176\n",
      "240\n",
      "324\n",
      "340\n",
      "176\n",
      "176\n",
      "340\n",
      "324\n",
      "176\n",
      "324\n",
      "176\n",
      "176\n",
      "240\n",
      "240\n",
      "240\n",
      "176\n",
      "176\n",
      "176\n",
      "176\n",
      "176\n",
      "240\n",
      "176\n",
      "240\n",
      "176\n",
      "324\n",
      "176\n",
      "176\n",
      "240\n",
      "240\n",
      "240\n",
      "340\n",
      "176\n",
      "240\n",
      "324\n",
      "324\n",
      "176\n",
      "176\n",
      "176\n",
      "240\n",
      "176\n",
      "240\n",
      "340\n",
      "176\n",
      "340\n",
      "win ratio:   0.27 mean:  -0.50, sigma:   0.71\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean, stdev\n",
    "\n",
    "def optimal_policy(state):\n",
    "    return Utility.findUtilityValue(pi_star, state)\n",
    "\n",
    "def run_one_episode(policy, max_steps=-1):\n",
    "    # create game board with 2 random tiles  \n",
    "    mdp = GameEnvironment(2, 16)\n",
    "    state = mdp.reset()\n",
    "    # mdp.render()\n",
    "\n",
    "    total_reward = 0.0\n",
    "    done = False\n",
    "    step = 1\n",
    "    \n",
    "    while not done:\n",
    "        next_action = policy(state)\n",
    "        if next_action == None:\n",
    "            next_action = Action(randint(1,4))\n",
    "            # print(f\"random action: {next_action}\")\n",
    "            \n",
    "        state, done, reward, info = mdp.step(next_action)\n",
    "        # print(state, next_action, done, reward, info)\n",
    "        total_reward += reward\n",
    "        if max_steps != -1:\n",
    "            step += 1\n",
    "            if step > max_steps:\n",
    "                break\n",
    "    return total_reward, mdp.has_won(mdp.get_state())\n",
    "\n",
    "def measure_performance(policy, nr_episodes=10):\n",
    "    N = nr_episodes\n",
    "    print('statistics over', N, 'episodes')\n",
    "    all_rewards = []\n",
    "    for n in range(1, N+1):\n",
    "        episode_reward, has_won = run_one_episode(policy, max_steps=100)\n",
    "        # print('episode:', n, 'reward:', episode_reward)\n",
    "        all_rewards.append([episode_reward, has_won])\n",
    "    \n",
    "    win_ratio = 0\n",
    "    for x in all_rewards:\n",
    "        if x[1]:\n",
    "            win_ratio += 1\n",
    "    win_ratio = (win_ratio / len(all_rewards)) * 100\n",
    "    \n",
    "    print('win ratio: {:3}%; mean: {:6.2f}, sigma: {:6.2f}'.format(win_ratio, mean(all_rewards[0]), stdev(all_rewards[0])))\n",
    "    print()\n",
    "    for n, el in enumerate(all_rewards, 1):\n",
    "        print('ep: {:3d}, has won: {:6}, total reward: {:5.2f}'.format(n, el[1], el[0]))\n",
    "\n",
    "measure_performance(optimal_policy, nr_episodes = 100)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da3ec9ba63dac011d7c2149d03a658e24415e076227cdc43197121aae5b74ad2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
